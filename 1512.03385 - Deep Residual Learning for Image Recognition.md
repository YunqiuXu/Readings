## 1512.03385 - [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
&hearts; DL , CV , CNN, Recognition
+ These days I'm trying to perform hand detection via faster R-CNN, current base model is VGG16, the performance can be improved if I replace it with ResNet
+ ResNet152(ImageNet 2015 + CVPR 2016) is a very deep model and there has been a lot of related notes. The networks are easier to optimize, and can gain accuracy from considerably increased depth. 
+ Problems of deeper network
  + Vanishing / exploding gradients
  + Degradation: adding more layers will lead to higher training error
    + One solution of degradation: add "identity mapping" layers, and the other layers ar ecopied from the learned shallower model. However it's not easy to fit desired underlying mapping directly
    + This paper use deep residual learning framework to tackle degradation
+ Deep residual learning framework: 
  + H(x) denotes the desired underlying mapping, which is hard to get directly
  + To make this easier, we compute $F(x) := H(x) - x$ first, then $H(x) := F(x) + x$
 Â + $F(x) + x$ can be realized by feedforward NN with "short connections" $\rightarrow$ residual block
    + Short connections are those skipping one or more layers
    + We perform identity mapping on short connections
    + The outputs of short connections(x) are then added to the outputs (F(x)) of common stacked layers
+ Archetecture:
  + Build the plain network
    + For the same output feature map size, the layers have the same number of filters
    + If the feature map size is halved(1/2), the number of filters is doubled $\rightarrow$ preserve the time complexity per layer
    + Only one fully-connected layer
    + Fewer filters and lower complexity than VGG16
  + Insert short connections into plain network
    + If the dimensions are same, the identity can be inserted directly
    + If not(the dimensions increase), we use linear projection to match the dimensions
  + 2 kinds of building blocks in this paper
    + ResNet-34
    + Bottleneck building block for ResNet-50/101/152
+ Implementation details
  + Scale the images to 256x480 as augmentation
  + Cut 224x224 crop, with the per-pixel mean subtracted
  + Standard color augmentation
  + Batch normalization between convolution and activation: alleviate gradient vanishing / exploding
  + SGD
  + No dropout
