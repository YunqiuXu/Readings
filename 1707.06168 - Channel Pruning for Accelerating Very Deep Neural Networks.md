## 1707.06168 - [Channel Pruning for Accelerating Very Deep Neural Networks](https://arxiv.org/abs/1707.06168)

+ Other ref:
    + http://www.cnblogs.com/zhonghuasong/p/7640908.html
    + http://blog.csdn.net/qq_21970857/article/details/77479164
    + https://github.com/yihui-he/channel-pruning 
+ Introduction:
    + 3 ways to accelerate CNN:
        + Optimized implementation: [FFT](https://arxiv.org/abs/1412.7580)
        + Quantization: [BinaryNet](https://arxiv.org/abs/1602.02830)
        + Structured simplification: **this work**
    + Structured simplification:
        + tensor factorization: factorizes a convolutional layer into several efficient ones
        + sparse connection: deactivate connections between neurons or channels
        + channel pruning: **this work**, reduces number of channels in each layer
    + Challenge of pruning channels: removing channels in one layer may change the input of the following layer.
+ Method: 
    + Given a trained CNN model, each layer is pruned by minimizing reconstruction error on its output feature maps
    + Solve minimization problem:
        + Channels selection: based on LASSO, figure out most representative ones, remove the others
        + Feature map reconstruction: reconstruct outputs with remaining channels with linear least squares
    + Approximate network layer-by-layer, with accumulated error accounted
+ Result:
    + 5x faster VGG16 with 0.3% error increasing
    + 2x faster ResNet with 1.4% error increasing
    + 2x Xception with 1.0% error increasing
