# 1609.05140 - The Option-Critic Architecture

![2018-04-23 20-10-38屏幕截图.png-35.8kB][1]

+ **Yunqiu Xu**
+ Overview:
    + End-to-end HRL
    + Only need to specify the number of options
    + Do not need additional reward function or sub-goals
    + Option is similar to sub-task

---

## 1. Introduction
+ Why use temporal abstraction: can represent knowledge about action courses that happen at different time scales
+ Classical option framework (**Sutton 1999, Precup 2000**) : 定义不同time scales的action courses, 用于无缝learning and planning
    + Markovian option $\omega \in \Omega = (I_{\omega}, \pi_{\omega}, \beta_{\omega})$
    + $I_{\omega} \in S$ : initiation state set
    + $\pi_{\omega}$ : intra-option policy, similar to sub-task, but no need for inner reward function
    + $\beta_{\omega} : S \rightarrow [0,1]$ : termination function, take a state as input, output whether this option is ended
    + **Assumption $\forall s \in S, \forall \omega \in \Omega : s\in I_{\omega}$** , i.e. every option can be used everywhere, i.e. 对任意状态, 不存在不可用的选项, 无非是分数可能差点  
    + **思考: 训练termination function会不会带来额外的bias**

+ Challenges for existing work: 
    + Learning with subgoals maybe expensive
    + 学习单个任务很慢: 需要重用similar tasks中学到的options, 然而因为单个任务没什么与之相似的任务, 学习速度很慢
+ Our work: option-critic
    + 同时学习intra-option policies, termination functions and over-option policy
    + 仅仅需要制定number of desired options
    + 不需要subgoals, additional rewards以及demonstrations
    + 也可以加入额外的reward

## 2. Learning Options

+ 对于某个状态选择某个option $\omega \in \Omega$的Q函数
![2018-04-23 21-14-56屏幕截图.png-6.4kB][2]
    + 这个是master选择option的Q函数, 状态$s$已经确定, 选择哪个$\omega$需要我们进行评估
+ 对于某个状态选择某个option, 在这个option下选择某个动作 $Q_U(s,\omega,a) : S \times \Omega \times A \rightarrow R$ 的Q函数
![2018-04-23 21-15-09屏幕截图.png-7.1kB][3]
    + 这个是option选择动作的Q函数, $s$和$\omega$都已经确定, 选择哪个action需要我们进行评估
    + 另一种表示方法将其看作 expectation of next states, 有助于进行简化(只需要计算$Q_{\Omega}$即可)
![2018-04-23 22-08-08屏幕截图.png-4.9kB][4]

+ 其中 $U(\omega, s') : \Omega \times S \rightarrow R$ 为"option utality function", 这里 $s'$ 为在某个状态 $s$ 下选用动作 $a$ 到达的下一个状态
    ![2018-04-23 21-15-20屏幕截图.png-6.5kB][5]
    + $U$不是Q函数, 而用于选择两种状态
    + 若$\beta_{\omega}(s') = 1$, 则该option在这个状态$s'$终止 $\rightarrow$ $U(\omega, s') = V_{\Omega}(s')$, 计算TD value
    + 若$\beta_{\omega}(s') = 0$, 则该option $\omega$还未终止, 继续进行公式(1) $\rightarrow$ $U(\omega, s') = Q_{\Omega}(s',\omega)$, 计算Q value

+ 后面还对这些公式的梯度进行了推导, 并引入了advantage function
    + 某些情形下，每个动作的总回报都不为负
    + 那么所有的梯度值都大于等于0，此时每个动作出现的概率都会提高
    + 这在很大程度下减缓了学习的速度，而且也会使梯度的方差很大
    + 因此需要对reward使用某种标准化操作来降低梯度的方差$\rightarrow$ 减去baseline
    + baseline为TD value时即为advantage function
$$A_{\Omega}(s', \omega) = Q_{\Omega}(s', \omega) - V_{\Omega}(s')$$

## 3. Algorithms and Architecture

![2018-04-23 21-41-15屏幕截图.png-47.5kB][6]

+ Architecture:
    + Actor: intra-option policies, termination functions and oper-options policy
    + Critic: $Q_U, A_{\Omega}$
    + 若$\pi_{\Omega}$是基于贪婪的, 则根据公式(2), 可得到one-step off-policy update target $g_t^{公式(1)}$ (update target of $Q_{\Omega}(s,\omega)$): 
![2018-04-23 21-56-22屏幕截图.png-14.5kB][7]

+ Algorithm
    + 这里$\alpha, \alpha_{\theta}, \alpha_{\theta花体}$ 分别为critic, intra-option policies以及termination function的学习速率
    + 对于某一个状态$s$, 使用over-options policy $\pi_{\Omega}$选择一个option $\omega$
    + 在该option终止前, 每次基于其选择一个动作$a$, 然后先进入option evaluation阶段
    + Options evaluation:
        + 首先初始化$\delta$为获得的reward和估计的Q value的差值
        + 若$s'$非终止状态, $\delta$加上在该状态下继续选择该option $\omega$的Q估计值
        + 注意若$s'$为终止状态不会跳过这个if判断, 而是让$\delta$加上Q估计的最大值 (选择在这个状态下能最大化Q值的option), 相当于选了一个新的"最佳的"option
        + 将该option $\omega$在该状态$s$下选择该动作$a$的Q值 (**intra-option**) 表示为
![2018-04-27 17-09-53屏幕截图.png-19.7kB][8]
这里就和DQN很像了, 区别在Q估计值 $Q_{\Omega}$ 为对option的选择
    + Options improvement: 对$\omega$及这个option的termination function的参数进行更新
    + 若$s'$为终止状态, 跳出循环, 重新选择一个option
    + 既学习$Q_U$又学习$Q_\Omega$很费时间, 由于$Q_U$为expectation of next states (见公式(2)), 可以只学习$Q_{\Omega}$, 然后根据$Q_{\Omega}$求取$Q_U$的估计 $\rightarrow$ **在ALE的实验中用了这个**

![2018-04-23 22-00-13屏幕截图.png-74.9kB][9]

## 4. Experiment
+ Navigate room: 检测在忽然改变(定位目标改变)的环境中恢复的能力
![2018-04-23 22-29-22屏幕截图.png-60.3kB][10]
![2018-04-23 22-29-36屏幕截图.png-25.5kB][11]

+ Pinball:
![2018-04-23 22-31-49屏幕截图.png-43.2kB][12]
![2018-04-23 22-32-07屏幕截图.png-50.5kB][13]

+ ALE: 注意之前对$Q_U$计算的简化
    + 用DNN表示intra-option policies以及termination functions, 同时用其计算critic
![2018-04-23 22-33-13屏幕截图.png-37.5kB][14]
    + 防止gradient shrink: 向advantage function加一个很小的term $\xi = 0.01$, 这样做类似regularization, 在advantage function接近optimal时令其仍然为正
$$A_{\Omega}(s,\omega) + \xi = Q_{\Omega}(s, \omega) - V_{\Omega}(s) + \xi$$
![2018-04-23 22-38-11屏幕截图.png-132.4kB][15]

## 5. Summary
+ Option-critic
    + Goal: learning faster, updating intra-option policies and termination functions slower $\rightarrow$ **之后的工作A2OC通过增加deliberation cost来延长每个option的时间**
    + 可以实现end-to-end HRL, 仅仅需要预先指定option的数量
    + 我理解为每个option都是一个独立的policy, 然后对于 $\pi_{\Omega}$ 根据 state 选 option 的过程其实就类似于 $\pi_{\omega}$ 根据state选action的过程
    + 不需要额外的reward和sub-goal, 也可以稍加修改来增加additional reward
    + 增加additonal reward可以激励 **"options that are temporally extended by adding a penalty whenever a switching event occurs"** , 这句不大理解, 是想要激励什么样的options???
+ Option-critic的不足
    + 本工作的一个**假设: 在任何地方都可以使用任何option, 但实际上会不会存在某些地方只能用某几种option呢, 比如工具包括厨具农具, 在餐厅只能在厨具中进行选择**
    + FeUDal Network中也提到了OC结构, 说是容易退化到以下两种情况:
        + 整个任务只使用一个option不选择别的, 相当于是学了单个network
        + 每一步都换一个option, 微调过多, 相当于把option当成了一个action function $\rightarrow$ **A2OC进行了一些提升**
        + 因此和OC不同, FeUDal Network的master policy是会为worker policy设定显式的目标去完成的

+ Further reading:
    + A2OC (A3C + OC) : [1709.04571 - When waiting is not an option: Learning options with a deliberation cost][16]
    + [1710.11089 - Eigenoption Discovery through the Deep Successor Representation][17]
    + [1711.03817 - Learning with Options that Terminate Off-Policy][18]
    + [1802.03236 - Learning Robust Options][19]

+ Implementations:
    + Basic OC:
        + Theano + Lasagne for ALE : https://github.com/jeanharb/option_critic
        + TF, with deliberation cost : https://github.com/bhairavmehta95/option-critic
        + TF implementation : https://github.com/yadrimz/option-critic
    + Extensions:
        + prioritized_option_critic on 4-rooms : https://github.com/YuejiangLIU/prioritized_option_critic
        + A2OC : https://github.com/jeanharb/a2oc_delib
        + Eigenoption-Critic : https://github.com/ioanachelu/EigenOption-Critic_SR
            


  [1]: http://static.zybuluo.com/VenturerXu/1rnx4l1kwzxnsl03uxjipfhg/2018-04-23%2020-10-38%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [2]: http://static.zybuluo.com/VenturerXu/xffvy7vuapehmqcv07575q7b/2018-04-23%2021-14-56%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [3]: http://static.zybuluo.com/VenturerXu/zdl34qdhkunhh3clom77793a/2018-04-23%2021-15-09%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [4]: http://static.zybuluo.com/VenturerXu/493evwfmhd7vpb6599zjzwmd/2018-04-23%2022-08-08%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [5]: http://static.zybuluo.com/VenturerXu/liuf97wke9gw9n76f1os89cs/2018-04-23%2021-15-20%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [6]: http://static.zybuluo.com/VenturerXu/oyaaced0bfi83eqnn4afj5q5/2018-04-23%2021-41-15%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [7]: http://static.zybuluo.com/VenturerXu/y3auc1wveno5ws2djtjhms7k/2018-04-23%2021-56-22%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [8]: http://static.zybuluo.com/VenturerXu/5f4qxr75opyvstozhmv2olym/2018-04-27%2017-09-53%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [9]: http://static.zybuluo.com/VenturerXu/m85yb44l84b8q5jpcs8zvowq/2018-04-23%2022-00-13%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [10]: http://static.zybuluo.com/VenturerXu/e760eg48gqnknebbi2f8192k/2018-04-23%2022-29-22%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [11]: http://static.zybuluo.com/VenturerXu/wau9gc4h1k4wrkn671gx3sus/2018-04-23%2022-29-36%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [12]: http://static.zybuluo.com/VenturerXu/rgnzrdai3fmjlnrjy4stjt0t/2018-04-23%2022-31-49%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [13]: http://static.zybuluo.com/VenturerXu/0vta4o4c1nflz0kzy6assdp8/2018-04-23%2022-32-07%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [14]: http://static.zybuluo.com/VenturerXu/trdfnb2crnin5egd6etpsl0l/2018-04-23%2022-33-13%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [15]: http://static.zybuluo.com/VenturerXu/n2cjp73g7e6eb7s036m4bi5p/2018-04-23%2022-38-11%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [16]: https://arxiv.org/abs/1709.04571
  [17]: https://arxiv.org/abs/1710.11089
  [18]: https://arxiv.org/abs/1711.03817
  [19]: https://arxiv.org/abs/1802.03236