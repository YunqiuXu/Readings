# 1703.00837 - Meta Networks

+ **Yunqiu Xu**
+ **Unfinished, to be continued**


-----

## 1. Introduction and Related Work

+ Challenge of traditional DNN: 
    + Needs large dataset
    + Can not learn continuously without forgetting previously learned patterns

+ Previous meta-learning:
    + Formulate the problem as two-level learning:
    + Slow learning for meta-level across the tasks: get general knowledge
    + Fast learning for base-level within one task (即之前MAML的内循环) : general knowledge can be transferred fastly

+ Our work : MetaNet
    + support meta-level continual learning
    + learn / generalize a new task from single example : **又是个"one-shot"?**

## 2. Meta Networks
+ Total goal : fast learning and generalization by processing higher-order meta information
+ Architecture : meta learner + base learner + external memory
    + **Why use external memory: for fast learning and generalization**

![2017-12-19 15-06-06屏幕截图.png-43.7kB][1]

+ Meta learner :
    + Goal: fast weight generation
    + Task agnostic, performed in abstract meta space, supports continual learning
    + Performs meta-knowledge acquisition across tasks
    + Take input task and meta info as input
    + Then perform fast parameterization for itself and base learner
    + In this way, meta learner can get new concepts from input task 

+ Base learner: 
    + Performs within each task by capturing the task objective
    + Provide the feedback (meta infomation) $\rightarrow$ explain its own status in this task space

+ The weights:
    + Standard slow weights: updated via RL
    + Task-level fast weights: updated within the scope of each task
    + Example-level fast weights: updated for a specific input example
    + **Note that fast weights are generated and integrated into both base and meta learner to shift their inductive bias**
+ Meta information : two types of loss functions
    + representation (embedding) loss: for building good representation learner criteria
    + main (task) loss: for input task objective

![2017-12-19 16-56-17屏幕截图.png-123.2kB][2]

### 2.1 Meta Learner

### 2.2 Base Learner

### 2.3 Layer Augmentation

![2017-12-19 16-55-25屏幕截图.png-45.5kB][3]

## 3. Experiment

![2017-12-19 16-57-55屏幕截图.png-70kB][4]
![2017-12-19 16-58-07屏幕截图.png-45.5kB][5]
![2017-12-19 16-58-15屏幕截图.png-57.4kB][6]

## 4. Conclusion and Future Work

+ MetaNet:
    + Goal: rapid generalization
    + Performs generic knowledge acquisition in a meta space
    + Shifts parameters / inductive biases via fast parameterization
    + Use gradients as meta-information: generic and problem independent

+ Future work:
    + More robust and expressive meta information
    + Better method to integrate slow and fast weights
    
+ My understanding:
    + 这个工作和MAML有类似之处, 目的为快速学习泛化新任务, 且达到 "one-shot" 效果
    + 训练的目的在于学习slow weights, 然后通过快速学习 fast weights 来适应新任务
    + 和MAML的不同在于MetaNet所需的 "外部记忆" 是啥, MAML的卖点在于不需要额外的参数以及 "model agnostic" , 这篇文章呢?
    + 实验主要还是基于 Omniglot 和 Mini-ImageNet, 即MAML监督学习任务部分
        + Omniglot 这篇文章和 MAML 差不多
        + Mini-ImageNet 这篇文章效果稍强于 MAML

            


  [1]: http://static.zybuluo.com/VenturerXu/vhilxigstoqyxycfobih49lw/2017-12-19%2015-06-06%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [2]: http://static.zybuluo.com/VenturerXu/1qbjmgm3fgmpdv986yg8t6d2/2017-12-19%2016-56-17%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [3]: http://static.zybuluo.com/VenturerXu/5y24ryv9s9smwpuehsrrvbnl/2017-12-19%2016-55-25%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [4]: http://static.zybuluo.com/VenturerXu/rkhvjhscsgxoepiclnvif9mb/2017-12-19%2016-57-55%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [5]: http://static.zybuluo.com/VenturerXu/cn49ytdv198618jh0e2fn78a/2017-12-19%2016-58-07%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [6]: http://static.zybuluo.com/VenturerXu/nz3t3m7gppdgwfrpf9hy0jwg/2017-12-19%2016-58-15%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png