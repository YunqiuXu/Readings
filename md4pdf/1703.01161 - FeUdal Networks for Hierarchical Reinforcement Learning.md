# 1703.01161 - FeUdal Networks for Hierarchical Reinforcement Learning

+ **Yunqiu Xu**

-----

## 1. Introduction
+ Challenges:
    + Long-term credit assignment
    + Sparse reward: another solution can be found in [1707.05300 - Reverse Curriculum Generation for Reinforcement Learning][1]
+ Our work
    + Get insight from [Feudal reinforcement learning (1993)][2] , generalize its principle
    + End-to-end differentiable neural network with two levels of hierarchy: Manager and Worker
    + Manager: operates at a lower temporal resolution, produces a meaningful and explicit goal for Worker to achieve
    + Worker: follow the goals by an intrinsic reward
    + No gradients are propagated between Manager and Worker $\rightarrow$ Manager receives learning signal from the environment alone
    + Worker tries to maximise intrinsic reward and Manager tries to maximise extrinsic reward

+ Advantage: 
    + Facilitate very long timescale credit assignment
    + Encourage the emergence of sub-policies associated with different goals set by the Manager


## 2. Related Work

+ Hierarchical RL:
![H-RL.png-289.9kB][3]

+ Feudal RL by Dayan and Hinton, 1993: treat Worker as sub-policy
![Feudal RL 1993.png-180.1kB][4]

+ Combine DL with predefined sub-goals:
    + [1604.07255 - A Deep Hierarchical Approach to Lifelong Learning in Minecraft][5]
    + [1604.06057 - Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation][6]
    + However sub-goal discovery was not addressed

+ Some non-hierarchical state-of-the-art on Montezuma’s Revenge: orthogonal to H-DRL, can be combined together
    + [1606.01868 - Unifying Count-Based Exploration and Intrinsic Motivation][7]
    + [1611.05397 - Reinforcement Learning with Unsupervised Auxiliary Tasks][8]

## 3. The Model

![Feudal Net 001.png-196.6kB][9]
![Feudal Net 002.png-216.2kB][10]

## 4. Experiment
+ Montezuma's Revenge
![2017-12-02 01-40-30屏幕截图.png-172.5kB][11]
+ Other Atari Games
![2017-12-02 01-41-26屏幕截图.png-352.1kB][12]
+ Memory Tasks on Labyrinth
![2017-12-02 01-42-41屏幕截图.png-147.4kB][13]
![2017-12-02 01-42-59屏幕截图.png-164.1kB][14]
+ Ablative Analysis
![2017-12-02 01-48-04屏幕截图.png-210.6kB][15]
![2017-12-02 01-46-49屏幕截图.png-193.3kB][16]
![2017-12-02 01-47-20屏幕截图.png-247.5kB][17]
+ Action Repeat Transfer
![2017-12-02 01-44-35屏幕截图.png-211.2kB][18]


  [1]: https://arxiv.org/abs/1707.05300
  [2]: https://papers.nips.cc/paper/714-feudal-reinforcement-learning
  [3]: http://static.zybuluo.com/VenturerXu/v3ph6qbhpptluxwnojc87ydc/H-RL.png
  [4]: http://static.zybuluo.com/VenturerXu/d7ql46dx1yofdalilerm4n0d/Feudal%20RL%201993.png
  [5]: https://arxiv.org/abs/1604.07255
  [6]: https://arxiv.org/abs/1604.06057
  [7]: https://arxiv.org/abs/1606.01868
  [8]: https://arxiv.org/abs/1611.05397
  [9]: http://static.zybuluo.com/VenturerXu/utjozfcbmi3f22dfdimkkhkf/Feudal%20Net%20001.png
  [10]: http://static.zybuluo.com/VenturerXu/t6cuy34fsqhuyg3tyapwf3rb/Feudal%20Net%20002.png
  [11]: http://static.zybuluo.com/VenturerXu/exjqj94v7yc318s1wctsiq9h/2017-12-02%2001-40-30%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [12]: http://static.zybuluo.com/VenturerXu/lj9f4hgf52m9c3gffc9xep0z/2017-12-02%2001-41-26%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [13]: http://static.zybuluo.com/VenturerXu/18rt94khzz7cv57u7xww1g3k/2017-12-02%2001-42-41%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [14]: http://static.zybuluo.com/VenturerXu/oipvy625noee15ei615cp4wx/2017-12-02%2001-42-59%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [15]: http://static.zybuluo.com/VenturerXu/6hjpd65rmwqxicpm1pm71d2u/2017-12-02%2001-48-04%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [16]: http://static.zybuluo.com/VenturerXu/x6y5htp1laxose8o20bhay9m/2017-12-02%2001-46-49%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [17]: http://static.zybuluo.com/VenturerXu/yyjfkfq86h7kmrz7omwtbipk/2017-12-02%2001-47-20%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [18]: http://static.zybuluo.com/VenturerXu/s0p03eh5tl5vk8xfy84843u5/2017-12-02%2001-44-35%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png