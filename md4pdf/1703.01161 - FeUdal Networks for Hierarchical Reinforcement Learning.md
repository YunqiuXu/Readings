# 1703.01161 - FeUdal Networks for Hierarchical Reinforcement Learning

+ **Yunqiu Xu**

-----

## 1. Introduction
+ Challenges:
    + Long-term credit assignment
    + Sparse reward: another solution can be found in [1707.05300 - Reverse Curriculum Generation for Reinforcement Learning][1]
+ Our work
    + Get insight from [Feudal reinforcement learning (1993)][2] , generalize its principle
    + End-to-end differentiable neural network with two levels of hierarchy: Manager and Worker
    + **Manager network** : 
        + operates at a lower temporal resolution
        + produces a meaningful and explicit goal from a latent state-space
        + select latent goals for Worker, try to maximise **extrinsic reward**
    + **Worker network** : 
        + operates at a higher temporal resolution
        + follow the goals by an intrinsic reward
        + produces primitive actions, try to maximise **intrinsic reward**
    + No gradients are propagated between Manager and Worker $\rightarrow$ **Manager receives learning signal from the environment alone**

+ Advantage: 
    + Facilitate very long timescale credit assignment
    + Encourage the emergence of sub-policies associated with different goals set by the Manager


## 2. Related Work

+ Hierarchical RL:
![H-RL.png-289.9kB][3]

+ Feudal RL by Dayan and Hinton, 1993: treat Worker as sub-policy
![Feudal RL 1993.png-180.1kB][4]

+ Combine DL with predefined sub-goals:
    + [1604.07255 - A Deep Hierarchical Approach to Lifelong Learning in Minecraft][5]
    + [1604.06057 - Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation][6]
    + However sub-goal discovery was not addressed

+ Some non-hierarchical state-of-the-art on Montezuma’s Revenge: orthogonal to H-DRL, can be combined together
    + [1606.01868 - Unifying Count-Based Exploration and Intrinsic Motivation][7]
    + [1611.05397 - Reinforcement Learning with Unsupervised Auxiliary Tasks][8]

## 3. The Model

### 3.1 Overview of Forward dynamics

+ Both Manager and Worker are recurrent
    + Manager: 
        + Receieve state(transformed by CNN) from environment 
        + Compute latent state $s_t$
        + Output a goal $g_t$
    + How to train Manager to get $g_t$ : transition policy gradient 
    + Worker: 
        + Receieve both state from environment and goal set by the Manager
        + Produce actions
    + How to train Worker : intrinsic reward to produce actions that cause these goal directions to be achieved

![Feudal Net 001.png-196.6kB][9]

+ $Eq. 1$ :
    + Manager and worker share a perceptual module
    + Take an observation from env $x_t$
    + Compute a shared intermediate representation $z_t$
    + $f^{percept}$ : CNN
+ $Eq. 2$ :
    + compute the implicit states for Manager to compute goals
    + $f^{Mspace}$ : FC layer
+ $Eq. 3$ :
    + Compute the internal states $h^M$ and goals for Manager
    + $f^{Mrnn}$ : dilated LSTM
        + Operates at lower temporal resolution than the data stream
        + More details : [Yu & Koltun, 2015, Multi-Scale Context Aggregation by Dilated Convolutions][11]

+ $Eq. 4$ : 
    + Goal embedding
    + $w_t \in R^k$ is embedding vector mapped from $g_t$ via a linear projection $\phi$
    + During implementation:
        + $k = 16$
        + $\epsilon$ : prob at each step to emit a random goal

+ $Eq. 5$: 
    + $h^W$ : internal states for Worker
    + $U_t \in R^{|a| \times k}$ is the output of worker, an embedding for action 
    + $f^{Wrnn}$ : standard LSTM

+ $Eq.6$ : Policy $\pi_t$ is computed from the combination of $w_t$ and  $U_t$


![Blank Diagram - Page 1.png-264.6kB][10]

+ **Why the output of Manager (goal) always influence the final policy**
    + $\phi$ has no bias $\rightarrow$ never produce constant non-zero vector
    + So the setup will never ignore Manager's input

### 3.2 Learning $\rightarrow$ Train Worker

+ FuN is fully differentiable $\rightarrow$ we can train it end-to-end using pg operating on actions taken by Worker

+ **Why we do not propagate gradient between Manager and Worker**
    + $g_t$ need to have semantic meaning $\rightarrow$ define the temporal resolution of the Manager
    + If we train Manager by gradients coming from the Worker
    + Manager's goals $g$ will not have **semantic meaning** but internal latent variables
    + **注意如果这里 $g$ 不存在 semantic meaning 的话在后面计算 Worker 的 intrinsic reward 的时候就会有问题**

+ So what we do instead:
    + Independently train Manager to predict advantageous directions (transitions) in state space
    + Then intrinsically reward the Worker to follow these directions

+ Thus the update rule of Manager can be: 
$$\nabla_{g_t} = A_t^M \nabla_{\theta}d_{cos}(s_{t+c} - s_t, g_t(\theta)) \space\space\space\space(7)$$
    + $A_t^M = R_t - V_t^M(x_t,\theta)$ : Manager's advantage function
    + $d_{cos}(s_{t+c} - s_t, g_t(\theta))$ : the cosine similarity of $s_{t+c} - s_t$ and $g_t(\theta)$
    + **The dependence of $s$ on $\theta$ is ignored here to avoid trival solutions**
    + $\nabla_{g_t}$ can be seen as "advantageous direction"

+ The intrinsic reward of Worker can be:
$$r_t^I = \frac{1}{c} \sum_{i=1}^c d_{cos}(s_t - s_{t-i}, g_{t-i}) \space\space\space\space(8)$$
    + So here we need to give $g_{t-i}$ semantic meaning
    + **此处存疑, semantic meaning of $g_t$ 到底啥意思**

+ **My understanding** :
    + 这里 $g_t$ 不仅仅是一个position或者 reward value, 我们将其 理解为到达目标的方向
    + 两个state相减即为agent的前进方向, 因此我们要尽可能最大化其与 $g_t$ 的余弦相似度
    + 即让你当前走的方向和到达目标的方向尽可能一致
    
+ Compared with old version (Dayan & Hinton 1993), we add an intrinsic reward for following the goals, but retaining the environment reward as well
    + This is similar to **regularization**
    + Worker is trained to maximize $R_t + \alpha R_t^I$
    + Method to train Worker : A2C
    $$\nabla_{\pi_t} = A_t^D \nabla_{\theta} log \pi(a_t|x_t;\theta) \space\space\space\space(9)$$
    + Here advantage function can be transformed as
    $$A_t^D = R_t + \alpha R_t^I - V_t^D(x_t;\theta)$$

### 3.3 Transition Policy Gradients $\rightarrow$ Train Manager
+ The update of Manager is with respect to a model of Worker's behavior
+ **Assumption : sub-policies are fixed duration behaviors**
+ $o_t = \mu(s_t, \theta)$ : Master need to learn high level policy to select which subpolicy to use
+ $\pi^{TP}(s_{t+c}|s_t) = p(s_{t+c}| s_t, o_t)$ : each sub-policy can be represented as transition distribution, here $s_{t+c}$ means end states of this sub-policy
+ So transition policy can be seen as the distribution over end states given start states
$$\pi^{TP}(s_{t+c}|s_t) = p(s_{t+c}| s_t,\mu(s_t, \theta))$$
+ Then we can use PG to train $\pi^{TP}$
$$\nabla_{\theta} \pi_t^{TP} = E\left[ (R_t - V(s_t)) \nabla_{\theta} log p(s_{t+c} | s_t, \mu(s_t,\theta)) \right] \space\space\space\space(10)$$

+ **Why we just need to use end state distribution of sub-policies**
    + Worker may follow a complex trajectory, and it's hard to compute PG by learning from these trajectories
    + If we know the end states of trajectories, we can skip Worker's behavior, and just follow the PG or predicted transition


## 4. Experiment

+ Goal: 
    + Check whether FuN learns non-trival, helpful and interpretable subpolicies and subgoals
    + Validate components of the architecture

### 4.1 Montezuma's Revenge
+ Try to get the key to go out the first room
+ For each timestamp, compute latent state $s_t$ and goal $g_t$
+ Then try to find a future state $s_f$ to maximize $d_{cos}(s_f - s_t, g_t )$ $\rightarrow$ make them more similar
+ From (a) we can see that FuN needs less states to maximize the goal
+ **From (b) FuN learns semantically meaningful sub-goals: we can interpret the tall bar as useful "milestones" (e.g. turning right then going down)**

![2017-12-02 01-40-30屏幕截图.png-172.5kB][12]



### 4.2 Other Atari Games
![2017-12-02 01-41-26屏幕截图.png-352.1kB][13]

### 4.3 Visual memorisation tasks in 3D environment
![2017-12-02 01-42-41屏幕截图.png-147.4kB][14]
![2017-12-02 01-42-59屏幕截图.png-164.1kB][15]

### 4.4 Ablative Analysis
![2017-12-02 01-48-04屏幕截图.png-210.6kB][16]
![2017-12-02 01-46-49屏幕截图.png-193.3kB][17]
![2017-12-02 01-47-20屏幕截图.png-247.5kB][18]

+ Action Repeat Transfer

![2017-12-02 01-44-35屏幕截图.png-211.2kB][19]

## 5. Discussion and Future Work
+ How we formulate sub-goals
    + Set sub-goals as directions in latent state space
    + If followed, sub-goals will be translated as meaningful behavioral primitives

+ Future work:
    + Deeper hierarchies: 这个可以看下 DDO 和 DDCO
    + Transfer / multi-task Learning: 这个可以结合下 MIL 和 MLSH, 用 meta-learning 训练合适的子任务, 然后对于新的任务只需要训练 master (i.e. 在合适的时间选择合适的子任务进行执行)


  [1]: https://arxiv.org/abs/1707.05300
  [2]: https://papers.nips.cc/paper/714-feudal-reinforcement-learning
  [3]: http://static.zybuluo.com/VenturerXu/v3ph6qbhpptluxwnojc87ydc/H-RL.png
  [4]: http://static.zybuluo.com/VenturerXu/d7ql46dx1yofdalilerm4n0d/Feudal%20RL%201993.png
  [5]: https://arxiv.org/abs/1604.07255
  [6]: https://arxiv.org/abs/1604.06057
  [7]: https://arxiv.org/abs/1606.01868
  [8]: https://arxiv.org/abs/1611.05397
  [9]: http://static.zybuluo.com/VenturerXu/utjozfcbmi3f22dfdimkkhkf/Feudal%20Net%20001.png
  [10]: http://static.zybuluo.com/VenturerXu/dw2qy4u07kw53ltgsgjvx2l2/Blank%20Diagram%20-%20Page%201.png
  [11]: https://arxiv.org/abs/1511.07122
  [12]: http://static.zybuluo.com/VenturerXu/exjqj94v7yc318s1wctsiq9h/2017-12-02%2001-40-30%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [13]: http://static.zybuluo.com/VenturerXu/lj9f4hgf52m9c3gffc9xep0z/2017-12-02%2001-41-26%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [14]: http://static.zybuluo.com/VenturerXu/18rt94khzz7cv57u7xww1g3k/2017-12-02%2001-42-41%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [15]: http://static.zybuluo.com/VenturerXu/oipvy625noee15ei615cp4wx/2017-12-02%2001-42-59%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [16]: http://static.zybuluo.com/VenturerXu/6hjpd65rmwqxicpm1pm71d2u/2017-12-02%2001-48-04%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [17]: http://static.zybuluo.com/VenturerXu/x6y5htp1laxose8o20bhay9m/2017-12-02%2001-46-49%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [18]: http://static.zybuluo.com/VenturerXu/yyjfkfq86h7kmrz7omwtbipk/2017-12-02%2001-47-20%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [19]: http://static.zybuluo.com/VenturerXu/s0p03eh5tl5vk8xfy84843u5/2017-12-02%2001-44-35%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png