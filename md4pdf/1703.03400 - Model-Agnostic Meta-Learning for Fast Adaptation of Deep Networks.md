# 1703.03400 - Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks
+ **Yunqiu Xu**

+ 2nd for this paper, try to understand both paper and code
+ Other reference:
    + https://www.jiqizhixin.com/articles/2017-07-20-4
    + http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/
    + https://github.com/cbfinn/maml

## 1. Introduction
+ The goal of meta-learning: train a model with some learning tasks, then it can solve new tasks with only a few samples
+ Our work: 
    + MAML: pretrain the model with a series of tasks
    + Then this model can be generated to new task with a small number of training samples / gradient steps on that task

+ Our advantage: see 6.Discussion and Future Work

## 2. MAML
### 2.1 Problem Setup
+ Goal: train a "meta learning" model on a set of tasks, then this model can adapt to a new task with only a few data / iterations $\rightarrow$ learn **as much as** possible with limited data
+ Model $f: x \rightarrow a$
    + $x$ : observations
    + $a$ : outputs
    + This model is like a "base model" which will be able to adapt to a lot of new tasks
+ General notion of task $T = \{L(x_1,a_1,...,x_H,a_H), q(x_1), q(x_{t+1}|x_t,a_t),H\}$ :
    + $L \rightarrow R$ : loss function
    + $q(x_1)$ : distribution over initial observations
    + $q(x_{t+1}|x_t,a_t)$ : transiion distribution
        + 对于监督学习, 不存在这个分布 : $H = 1$
        + 对于强化学习, $q(x_{t+1}|x_t,a_t)$ 代表某时间点观察值的分布, e.g. 初始观察值后观察值取自$q(x_{2}|x_1,a_1)$
    + $H$ : 
        + Episode length, model may generate samples of length H by choosing an output $a_t$ at each time $t$
        + For supervised learning, $H = 1$ and loss function $L(x_1,a_1)$ could be MSE or cross entropy

### 2.2 K-shot meta-learning:

+ Train model f:
    + Sample a new task $T_i$ from $p(T)$ (training taskset)
    + Learn $T_i$ :
        + Train model with K samples drawn from $q_i$
        + Get feedback $L_{T_i}$ from $T_i$
    + Test on new samples from $T_i$ and get test error
    + Improve model f : treat the test error on sampled tasks $T_i$ as the training error of meta-learning process

+ Test meta-learning:
    + Sample new task from $p(T)$ (testing taskset), try to adapt $f$ to this new task
    + Learn the model with K samples
    + Treat the performance as "meta-performance"
    
### 2.3 A MAML Algorithm
+ What our model does?
    + Be able to learn parameters of any standard model via meta-learning
    + Why: some internal representations are more transferrable
    + E.G. 我们可以通过一系列任务学到一个神经网络模型, 而非仅仅通过一个任务, 这样这个模型就比较容易迁移到类似的新任务上
    
+ How we learn
    + Learn a model that gradient-based learning rule can make rapid progress on new tasks drawn from $p(T)$ without overfitting
    + Find model parameters that are **sensitive** to changes of the task $\rightarrow$ small changes lead to large improvement (direction of gradient)
    + 为什么这样做: 之前的目标就是在样本/更新次数有限的情况下学到的东西尽可能多, 因此我们要尽量让每一点点小改变都能获得较大的提升
    + **此处存疑: 代码里该怎么体现"sensitive"**

![2017-11-27 16-00-06屏幕截图.png-48.9kB][1]

+ Algorithm 1:
    + $f_{\theta}$, when adapting to a new training task $T_i$, the model's parameter $\theta$ becomes $\theta_i'$ 
    + Then we learn $\theta_i'$ by gradient update:
    $$\theta_i' = \theta - \alpha \nabla_{\theta}L_{T_i}(f_{\theta})$$
        + 这里我理解为调整梯度的方向, 比如为了适应 $T_2$ 我们需要将梯度方向稍微上移
        + 经过多次梯度更新(内循环)后, 我们就可以学到比较适合 $T_2$ 的参数向量 $\theta_2'$
        + Step size $\alpha$ : fixed as a hyperparameter or meta-learned
    + After learning the parameter vectors for all tasks in training task set ($\theta_1', \theta_2', ... \theta_n'$), compute their test error to update $\theta$:
        + Meta objective
    $$ min_{\theta} \sum_{T_i \sim p(T)} L_{T_i}(f_{\theta_i'}) = \sum_{T_i \sim p(T)}L_{T_i}(f_{\theta - \alpha \nabla_{\theta}L_{T_i}(f_{\theta})})$$
        + SGD:
    $$\theta \leftarrow \theta - \beta \nabla_{\theta} \sum_{T_i \sim p(T)} L_{T_i}(f_{\theta_i'})$$
    
![2017-11-27 16-22-00屏幕截图.png-93.8kB][2]

+ This process involves a gradient through a gradient $\rightarrow$ an additional backward pass through f to compute Hessian-vector products

## 3. Species of MAML

![2017-12-06 16-06-58屏幕截图.png-179.4kB][3]

### 3.1 Supervised learning
+ Model is to predict output value given input value
+ For each training task $T_i$
    + $H = 1$ : single input and single output
    + $L =$ MSE or corss entropy
    + $q_i(x_1)$ : 因为不存在时序观察值, 这个分布就是监督学习训练集样本的分布
    + I think there is no $q_i(x_{t+1}|x_t,a_t)$
    + Then generate K samples x from $q_i(x_1)$, compute the error between predicted value a and ground truth y

### 3.2 RL
+ Model is to predict action $a_t$ given state $x_t$
+ For each training task $T_i$:
    + timestep $t \in \{1,...,H\}$
    + The initial state distribution $q_i(x_1)$: 为了训练这个子任务, 我们会从这个初始值分布选取K个起始点
    + Transition distribution $q_i(x_{t+1}|x_t,a_t)$: 
        + 对每个当前观察值及选取的动作, 未来观察值同样构成一个分布
        + 比如我选择吃一口饭, 接下来可能观察到 $\{饱了, 还饿 \}$等状态
        + 当然我们训练好$T_i$, 获得的未来状态可能就是选取最优动作后的结果了
    + For $T_i$ and its model parameter $\phi$, the loss function is 
    $$L_{T_i}(f_{\psi}) = -E_{x_t,a_t \sim f_{\psi, q_{T_i}}} \left[  \sum_{t=1}^HR_i(x_t,a_t)\right] \text{   (4)}$$
        + As reward function is to maximize reward, in loss function we multiply "-1" to minimize the value
        + Here $\phi$ is $\theta_i'$ , which we mentioned before
    + Why in step 8 we sample trajectories using $f_{\theta_i'}$ instead of $f_{\theta_i}$ : PG is on-policy, thus each additional gradient updateing during the adaption of $f_{\theta}$ need to sample from current policy

## 4. Related work
![2017-11-27 21-28-20屏幕截图.png-173kB][4]

+ RNNs as learners: MANN
    + Search space includes all conceivable ML algorithms
    + Moves the burden of innovation to RNNs
    + Ignors advances achieved in ML by humans
    + The results are not good

+ Metric learning: Siamese nets, matching nets
    + Learn a metric in input space
    + Specialized to one/few-shot classification
    + Can't use in other problems

+ Optimizer learning: meta-learner LSTM
    + Learn parameter update given gradients (search space includes SGD, RMSProp, Adam etc)
    + Applicable to any architecture / task
    + But we can achieve better performance with MAML

## 5. Experimental evaluation
+ Questions need to be answered:
    + Can MAML enable fast learning of new tasks
    + Can MAML be used for meta-learning in multiple different domains: SL / RL ...
    + Can a model learned with MAML continue to improve with additional gradient updates and/or examples

+ An oracle work: 
    + Receives the identity of the task (which is a problem-dependent representation) as an additional input, 
    + Thus oracle will be an upper bound on the performance of the model
    + 我们主要对比用或不用MAML的情形, 越接近oracle代表效果越好

### 5.1 Regression
+ 模拟sin曲线
+ Each task: 
    + The shape of curve varies by amplitude and phase
    + Input and output of a sine wave
    + Data points sampled from [-5.0, 5.0]
    + Loss: MSE
+ Model architecture: NN with 2 hidden layers, 40 hidden nodes, ReLU
+ Training: 
    + K = 10
    + \alpha = 0.01
    + Adam
    + After all training tasks, we get a pretrained model
+ Testing:
    + Fine-tune the pretrained model with K test samples and a number of GDs
    
+ Result
    + Left is pretrained with MAML, right is without MAML
    ![2017-12-06 16-10-25屏幕截图.png-144.1kB][5]
    ![2017-12-06 16-14-29屏幕截图.png-194.2kB][6]
    + Even with only 5 datapoints the fitting curve is nice
    + When all the points are in one half, the model can still infer the shape of the other half $\rightarrow$ model the periodic nature
    + Quantitative results: improve with addition gradient steps
        + 但素, sin曲线这种简单任务好像也就是再多迭代一个循环的事情 :)
    + 总之对Regression可以用很少的样本/循环finetune, 不会overfitting

### 5.2 Classification
+ Vinyals et al. 2016 Matching networks for one shot learning
+ Tasks : Few-shot image recognition

+ Datasets
    + Omniglot: 
        + 20 instances of 1623 characters from 50 different alphabets
        + Downsampled to 28 * 28
        + 1200 characters for training, remain for testing
        + Augmentation: degree retations
    + MiniImagenet: 64 training classes, 12 validation classes, and 24 test classes

+ Evaluation : N-way classification 
    + Select N unseen classes
    + Provide the model with K different instances of each of the N classes
    + Evaluate the model’s ability to classify new instances within the N classes
    
+ Model architecture
    + 4 modules, each module:
        + 3*3 conv, 64 filters (32 filters for MiniImagenet)
        + Batch normalization
        + ReLU nonlinearity
        + Strided convolutions (2*2 max-pooling for MiniImagenet)
    + A non-conv network for comparison: 256-128-64-64, BN, ReLU
    + Loss: cross entropy

+ **Comparision result : see 4. related work**
+ We also compare the performance between first order and second order derivatives
    + From $\theta \leftarrow \theta - \beta \nabla_{\theta} \sum_{T_i \sim p(T)} L_{T_i}(f_{\theta_i'})$ we can find that there is second order derivatives, which maybe computational expensive
    + Thus we compare it with first-order approximation
        + Compute the meta-gradient at the post-update parameter values $\theta_i'$
    + Result: 
        + **1st order is similar to 2nd order, less need to use 2nd derivatives**
        + Improvement of MAML comes from gradients of the objective at the post-update parameter values
        + Not 2nd derivative for differentiating through the gradient update
        + The use of ReLU make most of 2nd derivatives close to 0
    
### 5.3 Reinforcement Learning
+ Dual et al. 2016 Benchmarking deep reinforcement learning for continuous control
+ Model architecture
    + 2 hidden layers, 100 hidden nodes, ReLU
    + Vanilla PG + TRPO
    + Use finite differences to compute Hessian-vector products for TRPO: avoid computing third derivatives
    
+ Comparison: 
    + policy inited with MAML
    + policy inited with randomly weights
    + oracle policy

+ 2D Navigation
    + Goal: move to goal positions in 2D
    + Observation: current 2D position
    + Action: velocity commands
    + Reward: negative squared distance
    + When to terminate: too close to the goal, or H = 100
    + Comparison: adaptation to a new task with up to 4 gradient updates, each with 40 samples
    ![2017-12-06 16-17-50屏幕截图.png-104.4kB][7]

+ MuJoCo Simulation:
    + Goal velocity experiment: the reward is the negative absolute value between the current velocity of the agent and a goal
    + Goal direction experiments: the reward is the magnitude of the velocity in either the forward or backward direction
    + Result : MAML learns a model that can quickly adapt its velocity and direction with even just a single gradient update, and continues to improve with more gradient steps
    + 本文中经过MAML预训练的模型强于随机初始化, 不过在Parisotto et al 2016.的工作中, 也有预训练不如随机初始化的情况
    + Note that random baseline curves for game velocity are removed (worse return)
    ![2017-12-06 16-19-33屏幕截图.png-138.9kB][8]


## 6. Discussion and Future Work
+ MAML can be treated as an initialization method to get pretrained  model that is easy to fine-tune
+ Benefits:
    + Simple, does not introduce any learned parameters for meta-learning
    + Can be applied to regression / classification / RL
    + Adaptation on new tasks with few shot / updates
+ Future work:
    + Generalize meta-learning technique to apply to any problem and any model $\rightarrow$ 1709.04905 - One-Shot Visual Imitation Learning via Meta-Learning
    + Apply to multi-task
     
 


  [1]: http://static.zybuluo.com/VenturerXu/qwywwlsg9fbaf3p2vnd9mve9/2017-11-27%2016-00-06%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [2]: http://static.zybuluo.com/VenturerXu/f4ghc4fgaljmr6p5xosob9y1/2017-11-27%2016-22-00%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [3]: http://static.zybuluo.com/VenturerXu/hg3jfer4y50648emsrt0t9p2/2017-12-06%2016-06-58%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [4]: http://static.zybuluo.com/VenturerXu/xgmwsr30ngef64hc873qhi47/2017-11-27%2021-28-20%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [5]: http://static.zybuluo.com/VenturerXu/39k84iikus17z2pgr0k5ww1y/2017-12-06%2016-10-25%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [6]: http://static.zybuluo.com/VenturerXu/16r34nbeggva5y39zz5iwuu1/2017-12-06%2016-14-29%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [7]: http://static.zybuluo.com/VenturerXu/ebzatk0u4tovrhmka8jzgsfm/2017-12-06%2016-17-50%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [8]: http://static.zybuluo.com/VenturerXu/luaavzm190k6pgxyxfw1w4se/2017-12-06%2016-19-33%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png