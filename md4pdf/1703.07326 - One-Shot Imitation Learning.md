# 1703.07326 - One-Shot Imitation Learning

+ **Yunqiu Xu**
+ Other reference:
    + https://zhuanlan.zhihu.com/p/27935902
    + https://blog.openai.com/robots-that-learn/
    + https://github.com/DanielTakeshi/Paper_Notes/blob/master/reinforcement_learning/One-Shot_Imitation_Learning.md

        
-----

## 1. Introduction
+ Challenges of imitation learning
    + careful feature engineering
    + large number of samples
+ Our goal: learn from very few demonstrations of some tasks, then generalize to new situations of these tasks without task-specific engineering
+ Our method:
    + Train a policy on a large set of tasks
    + Input:
        + current state
        + one demonstration that solves a different instance for the same task
    + Output: current controls
    + Use soft-attention for generalization
![oneshot-001.png-265.5kB][1]

## 2. Related Work
+ Main lines of imitation learning:
    + Behavioral cloning: performs supervised learning from observations to actions
    + Inverse reinforcement learning: reward function is estimated that explains the demonstrations as (near) optimal behavior
    + These 2 methods consider each skill separately $\rightarrow$ learn one skill can not accelerate the learning to imitate another skill
+ One-shot and few-shot learning:
    + Many of the afore-mentioned approaches are a form of meta-learning
    + Learn the algorithm itself $\rightarrow$ learning to learn 
+ Another related work is [1703.01703 - Third-Person Imitation Learning][2] , but not good enough: learning uses one demonstration on whatever new task is being considered
+ Our method relies on "soft attention" and sequence-to-sequence model
    + an attention model over the demonstration
    + an attention model over the current observation
        
## 3. One Shot Imitation Learning
+ $t \in T$ : a task in the distribution of tasks
+ $d \in D(t)$
    + $D(t)$ is the distribution of demonstrations of task $t$
    + $d$ is a demonstration in $D(t)$
    + $d = [(o_1, a_1), (o_2, a_2), ..., (o_T, a_T)]$ : a demonstration is a sequence of observations and actions
    + We assume $T$ is given $\rightarrow$ for each $t \in T$ we can get demonstrations from $D(t)$
+ $\pi_{\theta}(a|o,d)$ : policy
+ $R_t(d)$ : evaluation function
+ Objective: maximize the expected performance of the policy, where
the expectation is taken over tasks $t \in T$, and demonstra-
tions $d \in D(t)$
+ An example: block stacking
![2017-12-01 17-22-20屏幕截图.png-191.2kB][3]

## 4. Architecture
+ Particle reaching: 3 candidates
    + Plain LSTM
    + LSTM with attention
    + Final state with attention

+ Block stacking: 3 modules
    + Demonstration network: 
        + Input: the entire demonstration
        + Output: some embedding
        + This demonstration can be long, so they need to subsample for timestep
![2017-12-01 17-25-16屏幕截图.png-22.7kB][4]
    + Context network: 
        + Inpput: embedding from demonstration, and current state, 
        + Output: context embedding
        + the demonstration embedding is going to be fixed, so the input at time $t$ is the concatenated vector $(d, x_t)$
![2017-12-01 17-25-40屏幕截图.png-29.9kB][5]
    + Manipulation network: 
        + Input: embedding from context network
        + Output: the action to be taken
![2017-12-01 17-26-41屏幕截图.png-13.4kB][6]

## 5. Experiments
+ Particle Reaching
![2017-12-01 17-27-24屏幕截图.png-39.2kB][7]
+ Block stacking: Figure 9 - Figure 13



    


  [1]: http://static.zybuluo.com/VenturerXu/knd5jtb9trgk5ghz0ib3qk3l/oneshot-001.png
  [2]: https://arxiv.org/abs/1703.01703
  [3]: http://static.zybuluo.com/VenturerXu/zptd4bss0t4nuzzh3ysqll3w/2017-12-01%2017-22-20%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [4]: http://static.zybuluo.com/VenturerXu/o2ep4h78cx1bub7vj3mw0u4l/2017-12-01%2017-25-16%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [5]: http://static.zybuluo.com/VenturerXu/kn8s8b298pp0wx5ty1ylmyzh/2017-12-01%2017-25-40%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [6]: http://static.zybuluo.com/VenturerXu/m7xmvru5ve1ycet44gb5c1jg/2017-12-01%2017-26-41%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [7]: http://static.zybuluo.com/VenturerXu/7mydjdiytr9p07amddrb20c5/2017-12-01%2017-27-24%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png