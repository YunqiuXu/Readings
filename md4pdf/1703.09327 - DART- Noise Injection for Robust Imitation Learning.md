# 1703.09327 - DART: Noise Injection for Robust Imitation Learning

+ **Yunqiu Xu**
+ Other resource:
    + http://bair.berkeley.edu/blog/2017/10/26/dart/
    + https://github.com/DanielTakeshi/Paper_Notes/blob/master/reinforcement_learning/DART_Noise_Injection_for_Robust_Imitation_Learning.md
    
-----

## 1. Introduction
+ Challenges of imitation learning:
    + For behavior cloning (off-policy), need superviser's demonstration
    + For on-policy methods, need human superviser $\rightarrow$ computation burder
    
+ Our work:
    + Focus on **off-policy**, try to improve the performance of **behavior cloning**
    + Add noisy into supervisor's policy during demonstrating $\rightarrow$ demonstrate how to recover from errors
    + DART: Disturbances for Augmenting Robot Trajectories
        + Collect demonstrations with injected noise
        + Optimize the noise level to approximate the error of the robot’s trained policy during data collection

## 2. Related Work
+ Off-policy:
    + e.g. behavior cloning, ALVINN for self-driving
    + The robot passively observes the supervisor, then learns a policy mapping states to controls by approximating the supervisor’s policy
    + Limitation: 不好举一反三, 和教的有一点点不一样就傻逼了
+ On-policy:
    + e.g. DAgger, supervisor iteratively provides corrective feedback on the robot’s behavior
    + Alleviate the problem of compounding errors
    + Limitation:
        + Providing feedback : human supervisors
        + Safety : require the robot to visit potentially dangerous region
        + Computation : require retraining the policy from scratch after each round of corrections

## 3. Problem Statement
+ Policy $\pi_{\theta}$ : probability density over the set of trajectories of length $T$
    + $x$ : state
    + $u$ : action
    + $\xi = (x_0, u_0, x_1, u_1, ..., x_T, u_T)$ : trajectory, a finite sequence of $T$ pairs of states visited and corresponding control inputs at these states
$$p(\xi|\pi_{\theta}) = p(x_0)\prod_{t=0}^{T-1}\pi_{\theta}(u_t|x_t)p(x_{t+1}|u_t, x_t)$$


+ Imitation learning: 
    + Surrogate loss: the difference between controls, $l(u_1,u_2) = ||u_1 - u_2||_2^2$
    + Total loss: $J(\theta_1, \theta_2|\xi) = \sum_{t=0}^{T-1}l(\pi_{\theta_1}(x_t), \pi_{\theta_1}(x_t))$
    + Try to minimize expected surrogate loss along the distribution induced by the robot's policy
    + The distribution on trajectories and the cumulative surrogate loss are coupled $\rightarrow$ hard to optimize
$$min_{\theta} E_{p(\xi|\pi_{\theta})}J(\theta, \theta^*|\xi)$$

+ Transform to behavior cloning $\rightarrow$ off policy
    + Sample from the supervisor's distribution
    + Performs expected risk minimization on the demonstrations
    $$\theta^R = argmin_{\theta} E_{p(\xi|\pi_{\theta^*})} J(\theta, \theta^*|\xi)$$

+ Thus the performance of the policy $\theta^R$ can be written as the sum of covariate shift and the standard loss
![2017-12-02 16-30-56屏幕截图.png-22.5kB][1]

+ In this work, we focus on minimizing **covariate shift**

## 4. Off-Policy Imitation Learning with Noise Injection

![2017-12-02 20-19-52屏幕截图.png-31kB][2]

+ Robot tries to reach $X_G$, grey denotes the distribution over trajectories
    + Off-Policy: 
        + The supervisor (orange arrows), provides demonstrations
        + The robot (teal arrows), deviates from the distributions and incurs high error
    + On-Policy: 
        + Samples from the current robot's policy (light teal arrows) to receive corrective examples from the superviser
        + Computation expensive and unsafe
    + DART:
        + Injects noise to widen supervisor's distribution $\rightarrow$ provide corrective examples
        + Off-policy but robust

+ DART:
    + $p(\xi|\pi_{\theta_R})$ is not known until the robot has been trained
    + We interatively sample from the superviser's distribution with current noise parameter $\psi_k$
![2017-12-02 20-49-35屏幕截图.png-15.3kB][3]
![2017-12-02 20-49-43屏幕截图.png-17.6kB][4]
    + Pseudo code
    ![2017-12-02 20-50-10屏幕截图.png-101kB][5]


## 5. Experiments
+ Questions:
    + Does DART reduce covariate shift as effectively as on-policy methods
    + How much does DART reduce the computational cost
    + How much does it decay the supervisor’s performance during data collection
    + Are human supervisors able to provide better demonstrations with DART

+ MuJoCo Locomotion
![2017-12-02 20-53-01屏幕截图.png-194.7kB][6]

+ Robotic Grasping in Clutter
![2017-12-02 20-53-46屏幕截图.png-223.3kB][7]

## 6. Conclusion
+ Add noise to broaden supervisor's demonstration
+ Try to make off-policy imitation learning (behavior cloning) more robust

            


  [1]: http://static.zybuluo.com/VenturerXu/s92e06puow1zpc82jagshm8a/2017-12-02%2016-30-56%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [2]: http://static.zybuluo.com/VenturerXu/yxus90t862kyi9p9q3y5bu88/2017-12-02%2020-19-52%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [3]: http://static.zybuluo.com/VenturerXu/c1p97pkt26cv7zpvubgo3j3v/2017-12-02%2020-49-35%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [4]: http://static.zybuluo.com/VenturerXu/yhwrd4xghuj3x585tbljsfux/2017-12-02%2020-49-43%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [5]: http://static.zybuluo.com/VenturerXu/didm5gfet4jxw2zs2o86xwms/2017-12-02%2020-50-10%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [6]: http://static.zybuluo.com/VenturerXu/ezp7tl6xwq61l3psxncje00g/2017-12-02%2020-53-01%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [7]: http://static.zybuluo.com/VenturerXu/mta9iuwyj67bbiw0298edaw9/2017-12-02%2020-53-46%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png