# 1704.03732 - Deep Q-learning from Demonstrations

+ **Yunqiu Xu**
+ DeepMind AAAI 2018的论文, 和之前在arxiv上看得有一些修改, 现在对imitation learning有一定了解后再看下
+ Here are some implementations which I can leverage:
    + https://github.com/go2sea/DQfD
    + https://github.com/nabergh/doom_dqfd

+ Further reading:
    + 1709.10089 - Overcoming Exploration in Reinforcement Learning with Demonstrations
    
---
## 1. Introduction

+ Challenges:
    + DRL: requires large data to train
    + Learn from simulation (sim-to-real): 
        + Hard to cast into real world
        + Hard to find accurate simulator

+ Our work: DQfD
    + Leverage demonstrations(previous control) to pretrain the agent
    + Then the agent can perform well from the start of learning, and continue to improve by using its self-generated data
    + **Combine TD loss and classification loss**:
        + Classification loss: imitate the desmonstrator
        + TD loss: learn a self-consistent value function
    + **Prioritized replay**: balance the ratio of demonstration and self-generated data

## 2. Background and Related Work
+ Background: MDP, different DQNs
+ Related work: DAGGER: 
    + Requires expert to be available (on-policy) during training
    + Does not combine imitation with RL
+ Related work: Deeply AggreVaTeD:
    + Same requirement as DAGGER
    + The expert must provide a value function as well as actions
    + Only does imitation, can not learn to improve
+ Similar work: RL with Expert Demonstraitons:
    + Similar to our work: combine TD and classification loss
    + Our difference: 
        + Our agent is pre-trained on the demonstration data, then use self-generated data
        + The use of prioritized replay mechanism

+ Similar work: Schaal 1996, Learning from Demonstration
    + Use demonstrations to pretrain agent
    + But do not use supervised loss
+ Related work: One-Shot Imitation Learning
    + Input the entire demonstration in addition to current state
    + A distribution of demonstrations with different initial and goal states
    + Can not learn to improve from demonstrations
+ Similar work: Accelerated DQN with Expert Trajectories (ADET)
    + Combine TD and classification loss
    + 使用已训练好的一个agent (expert) 来生成demonstration
    + Difference:
        + 使用交叉熵loss (我们用large margin loss)
        + 并未预训练agent使其在与环境的第一次互动时就表现良好

## 3. DQfD
+ Pre-training phase:
    + Performed before real system, uses demonstrations only
    + Goal: learn to imitate the demonstrator with a value function
    + Losses: combine Q-loss, classification loss and L2 loss

+ Combined loss:
    + Supervised loss: large margin
    $$J_E(Q) = max_{a \in A} [Q(s,a) + l(a_E,a)] - Q(s,a_E)$$
        + 为什么用这个不用交叉熵(前人): 对获得的value做下限制
        + $l(a_E,a)$ : $a_E$ is for expert, if $a_E = a$ , the value is 0, otherwise the value is positive
        + 使用这个loss可以保证其他动作的value总是比专家动作的value低一点点
        + **During training (self-generated data), supervised loss will not be used $\rightarrow$ $\lambda_2 = 0$**
    + Q loss: 
        + one-step or n-step
        + 仅仅使用分类loss也不行, 需要使用Q-network进行提升
    + L2 loss: for regularization
$$J(Q) = J_{DQ}(Q) + \lambda_1J_n(Q) + \lambda_2J_E(Q) + \lambda_3J_{L2}(Q)$$

+ Replay buffer $D^{replay}$:
    + Used to store demonstraton
    + Used in pre-training first, store expert's demonstration
    + During training, keep adding self-generated data, if full, **overwrite self-generated data**
    + **Expert's demonstration data will never change once added**
    + Prioritized mechanism: 
        + Control the relative sampling of demonstration versus agent data
        + Use different positive constants $\epsilon_a$ and $\epsilon_d$ to balance the ratio of self-generated data and demonstration

+ Pesudocode:
![2018-01-19 16-21-54屏幕截图.png-154.4kB][1]
    + 初始化时注意 $\tau$ 的定义: 用于判定何时更新target network
    + 2-7行: pretraining
    + 8-17行: training
        + 和前面的区别就在于多了自己生成的数据
        + 注意11行, 只会复写self-generated data, 之前加进去的demonstration不变
        + 注意13行不需要计算分类loss了

+ 和PDDDQN (综合之前的版本, 可参考Rainbow)的对比
    + $D^{replay}$ 中永久保持demonstration, 仅仅重写self-generated data
    + 预训练时仅仅用demonstration, 此时不与环境进行互动
    + Combined losses
    + Demonstration priority


## 4. Experiment
+ Environment: ALE
+ Evaluation:
    + Full DQfD with human demonstrations
    + PDD DQN
        + No demonstration
        + No pretraining
        + No supervised losses and regularization
    + Supervised imitation (BC), no environment interaction
        + Cross entropy loss
        + No TD loss $\rightarrow$ only learns from pre-training
    
+ Human demonstration:
    + Human playing : 3-12 times per game
    + During playing : log agent's state, actions, rewards and terminations
    + 5574-75472 transitions per game (very small demonstration dataset compared with AlphaGo and DQN)

## 5. Result and Discussion
+ On-line scores:
![2018-01-19 17-03-22屏幕截图.png-134.8kB][2]
    + Pretraining: DQfD is much better than PDD DQN at the beginning
        + **However, only naively adding (e.g. only pretraining or filling replay buffer) can not achieve similar performance**
    + Pritorized mechanism:
        + Even after pre-training, the agent still needs expert demonstration
        + The need of expert data grows when the game becomes more difficult, e.g. reaching new screens

+ Loss ablation analysis
![2018-01-19 17-08-01屏幕截图.png-37.8kB][3]
![2018-01-19 17-04-00屏幕截图.png-167.1kB][4]
    
+ Score for 11 games
![2018-01-19 17-07-13屏幕截图.png-139.3kB][5]

## 6. Summary and Future Work
+ DQfD:
    + Pretraining using demonstration, then train with both demonstration and self-generated data
    + Combined losses
    + Pritorized mechanism
    + Small demonstration set with good performance
+ Future work:
    + Derive more value from demonstrations:
        + Human learn from demonstration in a different way
        + Some information may be inaccessible for current system
    + Apply DQfD in continuous environment (classification loss $\rightarrow$ regression loss)
+ My review:
    + DQfD is a combination of RL and imitation learning
    + DQfD can be used for discrete env, for continuous env, see further work using DDPG
    

  [1]: http://static.zybuluo.com/VenturerXu/hv3n7lneow5zto4wae0ebnge/2018-01-19%2016-21-54%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [2]: http://static.zybuluo.com/VenturerXu/28kqginl6ckoibp48hr01ebi/2018-01-19%2017-03-22%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [3]: http://static.zybuluo.com/VenturerXu/t0pc3tmsuqd1u0r99zb25841/2018-01-19%2017-08-01%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [4]: http://static.zybuluo.com/VenturerXu/h0ed61dmifyacsyigffbomoh/2018-01-19%2017-04-00%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [5]: http://static.zybuluo.com/VenturerXu/rf98marij12sz2f6sntxwfd8/2018-01-19%2017-07-13%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png