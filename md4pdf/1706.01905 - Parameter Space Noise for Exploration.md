# 1706.01905 - Parameter Space Noise for Exploration

![2018-01-31 21-49-40屏幕截图.png-39.4kB][1]

+ **Yunqiu Xu**
+ OpenAI's NoisyNet
    + [OpenAI's blog][2]
    + [Talk on Vimeo][3]
+ Further readings which can be seen in my notes:
    + 1703.09327 - DART- Noise Injection for Robust Imitation Learning
        + Add noise to off-policy imitation learning
    + **1706.10295 - Noisy Networks for Exploration**
        + Very similar work by DeepMind
        + It seems that their work is more robust for that it's not restricted to Gaussian noise
    + 1710.02298 - Rainbow: Combining Improvements in Deep Reinforcement Learning
        + A combination of different DQN including NoisyNet

---
## 1. Introduction
+ Challenges: effective and efficient exploration
+ This work:
    + Investigate how parameter space noise can be combined with DRL to improve exploration behavior
    + Can be applied on both off-policy (DQN, DPPG) and on-policy (TRPO) methods

## 2. Parameter Space Noise for Exploration
+ Overview: 
    + Change current policy with Gaussian noise
$$\hat{\theta} = \theta + N(0, \sigma^2I)$$
    + The perturbed policy is sampled at the beginning of each episode
    + During an episode, the parameters of perturbed policy keep fixed

+ State-dependent exploration
    + Action space noise : independent with current state
    + Parameter space noise : 
        + Perturbed at the beginning of each episode and keep fixed within episode
        + **Ensure consistency in actions $\rightarrow$ dependence between state and action**
+ Use layer normalization between perturbed layers
+ Adapt the scale of the parameter space over time and relate it the variance in action space
+ This paper also shows parameter space noise for off-policy and on-policy methods, which I think is similar to DeepMind's work but is more abstract and complex

## 3. Experiments
+ Questions:

![2018-02-01 03-10-39屏幕截图.png-34.6kB][4]

### 3.1 Performance on Games

+ DQN results: discrete action space 

![2018-02-01 03-11-34屏幕截图.png-325.3kB][5]

+ DDPG results: continuous action space

![2018-02-01 03-12-07屏幕截图.png-170.5kB][6]

+ TRPO results: continuous, on-policy

![2018-02-01 03-13-01屏幕截图.png-112.5kB][7]

### 3.2 Explore Efficiency

![2018-02-01 03-15-10屏幕截图.png-68kB][8]

![2018-02-01 03-15-31屏幕截图.png-112.6kB][9]

![2018-02-01 03-15-47屏幕截图.png-148.3kB][10]

### 3.3 Comparison with Evolution Strategies

+ DQN with parameter space noise surpass ES on 15 out of 21 Atari games, while use 25 times less data

![2018-02-01 03-18-33屏幕截图.png-132kB][11]

## 4. Summary
+ 和DeepMind同期发的工作很类似, 也是通过给参数加噪音来改善exploration
+ DeepMind把他们的工作整合进Rainbow中, 可以看下

  [1]: http://static.zybuluo.com/VenturerXu/7dvdyhl2l6z14bds04jz58xy/2018-01-31%2021-49-40%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [2]: https://blog.openai.com/better-exploration-with-parameter-noise/
  [3]: https://vimeo.com/252185862
  [4]: http://static.zybuluo.com/VenturerXu/2unpknbgsvojhjdtg3oqqzea/2018-02-01%2003-10-39%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [5]: http://static.zybuluo.com/VenturerXu/ewi2s4madtmihy2cgm4gs0m0/2018-02-01%2003-11-34%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [6]: http://static.zybuluo.com/VenturerXu/1wycs50np0p94963gb3e674w/2018-02-01%2003-12-07%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [7]: http://static.zybuluo.com/VenturerXu/5y4wkhcoa2a1hanzgwg4r7jb/2018-02-01%2003-13-01%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [8]: http://static.zybuluo.com/VenturerXu/qwumyiz6ub84i3vz17ih47j9/2018-02-01%2003-15-10%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [9]: http://static.zybuluo.com/VenturerXu/qz8kznmns357isvddsrzp18r/2018-02-01%2003-15-31%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [10]: http://static.zybuluo.com/VenturerXu/xiftpp5y26vpjo3v3ok325ms/2018-02-01%2003-15-47%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [11]: http://static.zybuluo.com/VenturerXu/ggyusemd8h1a01mx3ypy4z6y/2018-02-01%2003-18-33%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png