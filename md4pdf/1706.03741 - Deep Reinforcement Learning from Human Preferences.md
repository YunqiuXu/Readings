# 1706.03741 - Deep Reinforcement Learning from Human Preferences

![2018-01-30 21-32-40屏幕截图.png-56.4kB][1]

+ **Yunqiu Xu**
+ Other reference
    + [OpenAI blog][2]
    + [机器之心][3]
    + [Two Minute Papers][4]
    + [An implementation][5]
+ Leverage **simple human preferences (give feedback)** to help learning $\rightarrow$ solve complex RL tasks without access to reward function

---
## 1. Introduction
+ Challenge
    + For many tasks, goals are complex, poorly-defined, hard to specify
    + Imitation learning : sometimes it's difficult to provide demonstration
    + Directly human feedback : require hundreds or thousands of hours of experience

+ Our work: 
    + Learn reward function from human feedback and then to optimize that reward function
    + Try to decrease the amount of human feedback to make it more practical
    + Goal: handle sequential decision problems without a well-specified reward function
        + 不一定需要提供完整的demonstration, 只要提供偏好就行(e.g. 哪个agent的表现看起来好一些)
        + 不一定需要专家级demonstration $\rightarrow$ 普通人也可提供feedback
        + 不需要提供过多或者过于复杂的回馈, 即使对于大型问题

![2018-01-30 21-55-33屏幕截图.png-45.7kB][6]

+ 同以往工作相比, 我们的主要贡献在于**极大简化了human feedback**, 例如对于工作Akrour et al, 2014. Programming by Feedback, feedback为对整体trajectory的偏好, 而我们的只要对short clip进行判断就好, 更高效

## 2. Preliminaries and Method
### 2.1 Problem Setting

+ Trajectory segment: a sequence of observations and actions $\sigma = ((o_0,a_0), (o_1,a_1), ..., (o_{k-1},a_{k-1})) \in (O \times A)^k$

+ Goal: let the agent produce trajectories which are preferred by human, while making as few queries as possible to human
+ By using human preference, we can try to evaluate the algorithm qualitatively, which can help to improve it quantitatively
+ 本文也给出了如何定量评估preference的方法: 

![2018-01-31 02-09-12屏幕截图.png-30.3kB][7]

### 2.2 Method
+ Policy $\pi : O \rightarrow A$
+ Estimated reward function $\hat{r} : O \times A \rightarrow R$
+ Update process:
![2018-01-31 01-34-50屏幕截图.png-111.6kB][8]

### 2.3 Details of the Method
+ How to optimize policy: A2C for Atari, TRPO for robot tasks
+ The shape of human judgement : A database $D$ of $(\sigma_1, \sigma_2, \mu)$
    + $\sigma_1, \sigma_2$ : two segments to compare
    + $\mu$ : distribution over $\{1,2\}$ indicating which segment is preferred $\rightarrow$
        + If one segment is more preferable $\rightarrow$ all mass of $\mu$ will be put on it
        + If segments are equally preferable $\rightarrow$ $\mu$ is uniform
        + If segments are incomparable $\rightarrow$ this pair will not be included in $D$
+ How to fit reward function with preference result
![2018-01-31 01-43-45屏幕截图.png-66.5kB][9]

+ How to select queries:
![2018-01-31 01-45-58屏幕截图.png-57.4kB][10]

## 3. Experiment
### 3.1 RL Tasks with unobserved rewards
+ Learn the goal by only asking human which of two trajectory segments is better

+ Atari result

![2018-01-31 01-58-29屏幕截图.png-294.6kB][11]

+ Mujoco result

![2018-01-31 01-56-19屏幕截图.png-251.7kB][12]

### 3.2 Novel Behaviors
+ Try to solve some complex behaviors where no reward function is available
    + The Hopper robot performing a sequence of backflips
    + The Half-Cheetah robot moving forward while standing on one leg
    + Keeping alongside other cars in Enduro

![2018-01-31 02-00-51屏幕截图.png-176.5kB][13]
        
### 3.3 Ablation Studies
+ Atari result

![2018-01-31 02-02-32屏幕截图.png-242.5kB][14]

+ Mujoco result

![2018-01-31 02-02-21屏幕截图.png-202kB][15]

## 4. Summary
+ 本文利用human preference来加速学习过程, 并尝试解决一些不容易定义reward function的任务
+ 虽然之前已经有关于利用preference或者未知reward function的工作, 本文的创新点在于尽可能设置有效且高效的preference selection, 在提升性能的同时尽可能减少对人类参与的需求
+ 和 imitation learning 相比, 本工作不需要提供"专家"的demonstration, 仅仅提供比较偏好即可, 在真实世界中我们往往难以获得足够多的demonstration, 该工作或许能提供一些思路 


  [1]: http://static.zybuluo.com/VenturerXu/29x1xmgi6l4rc6s2b75u6mpo/2018-01-30%2021-32-40%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [2]: https://blog.openai.com/deep-reinforcement-learning-from-human-preferences/
  [3]: https://www.jiqizhixin.com/articles/2017-06-14-3
  [4]: https://www.youtube.com/watch?v=WT0WtoYz2jE
  [5]: https://github.com/nottombrown/rl-teacher
  [6]: http://static.zybuluo.com/VenturerXu/wqgg4ytr1rdd0yhrmco5jgcg/2018-01-30%2021-55-33%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [7]: http://static.zybuluo.com/VenturerXu/p2pbox0e20t6x8z6bkehojrr/2018-01-31%2002-09-12%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [8]: http://static.zybuluo.com/VenturerXu/38c1doioqudtc4j7hzzcoibb/2018-01-31%2001-34-50%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [9]: http://static.zybuluo.com/VenturerXu/cdfg1nuvoyxf1pocfhqxvv2q/2018-01-31%2001-43-45%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [10]: http://static.zybuluo.com/VenturerXu/qkfn92wm4dtaod4z62vtps9o/2018-01-31%2001-45-58%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [11]: http://static.zybuluo.com/VenturerXu/u6m8jhpgrdqjcwjsp52gti1r/2018-01-31%2001-58-29%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [12]: http://static.zybuluo.com/VenturerXu/8sk9dmb9qnxakyh25a232fck/2018-01-31%2001-56-19%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [13]: http://static.zybuluo.com/VenturerXu/cchmtrpa4x3iqjauvcgkkg2b/2018-01-31%2002-00-51%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [14]: http://static.zybuluo.com/VenturerXu/7j75us13j5temjg7wju2xhdg/2018-01-31%2002-02-32%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [15]: http://static.zybuluo.com/VenturerXu/i3ni8a5sy0ic0ugx3ty4notw/2018-01-31%2002-02-21%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png