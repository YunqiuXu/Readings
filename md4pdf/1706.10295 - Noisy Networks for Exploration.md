# 1706.10295 - Noisy Networks for Exploration

![2018-01-31 15-55-45屏幕截图.png-63.4kB][1]

+ **Yunqiu Xu**
+ DeepMind's NoisyNet
    + Add parameytric noise to weights $\rightarrow$ introduce stochasticity to policy $\rightarrow$ efficient exploration
    + Has been integrated into Rainbow
+ Further readings, can be seen in my notes
    + 1703.09327 - DART- Noise Injection for Robust Imitation Learning
        + Add noise to off-policy imitation learning
    + 1706.01905 - Parameter Space Noise for Exploration 
        + Very similar work by OpenAI
    + 1710.02298 - Rainbow: Combining Improvements in Deep Reinforcement Learning
        + A combination of different DQN including NoisyNet

---

## 1. Introduction
+ Challenges: how to explore (introduce new behabiors) efficiently
    + Dithering pertubations such as $\epsilon$-greedy or entropy regularization
        + Random pertubations of agent's policy, e.g. trade-off exploration and exploitation with epsilon
        + **In every timestep, the noise is decorrelated**
        + Unefficient
    + Current methods' limitation:
        + Small state-action spaces
        + Linear function approximations
        + Not easy to be applied with more complicated system
    + A more structured method : add intrinsic motivation term to reward
        + Explicitly rewards novel discoveries
        + Limitation: 
            + Separate the mechanism of generalisation from exploration
            + Need to balance the importance between additional term and reward manually
            + Not data efficient
+ Our work: NoisyNet
    + Learn perturbations of weights to drive exploration
    + Key insight : a single change on weight can introduce effective changes in policy over multiple timesteps (**correlated noise**)
    + High level : introduce a randomised network for exploration
    + Requires only one extra parameter per weight
    + Can apply to PG methods such as A3C
+ Similar work by OpenAI: **1706.01905 - Parameter Space Noise for Exploration**
    + Add constant Gaussian noise to parameters
    + Our difference: 
        + Adapt the noise injection with time
        + Not restricted to Gaussian noise distributions
        + Can be adapted to any DRL such as DQN and A3C

## 2. NoisyNets for RL
+ What is NoisyNets:
    + NN whose weights and biases are pertubed by a parametric function of noise
    + These parameters are adapted with GD

+ Noisy layer $y = f_{\theta}(x)$
    + Take $x$ as input, then output noised data $y$
    + Here $x,y$ means weights or biases of general NN
    + Noisy parameters $\theta = \mu + \sum \odot \epsilon$
        + $\zeta = (\mu, \sum)$ : set of learnable parameter vectors
        + $\epsilon$ : zero-mean noise with fixed statistics (e.g. Gaussian distribution)

+ So for a general NN layer $y = wx + b$, the noise version can be:
$$y = (\mu^w + \sigma^w \odot \epsilon^w )x + \mu^b + \sigma^b \odot \epsilon^b$$
where $w,b$ are processed by noisy layer

+ Combine NoisyNets with DRL
    + NoisyNet agent: sample a new set of parameters after each step of optimisation
    + Between optimisation steps, this agent acts according to a fixed set of parameters
    + 每回合选择不同的参数集合, 但回合中保持参数不变

+ NoisyNet-DQN:
    + No $\epsilon$-greedy, the policy optimises value function greedily
    + FC layers of value function are parameterised as a noise network $\rightarrow$ processed per replay step
    + Factorised Gaussian noise
    + Action-value function $Q(x,a,\sigma;\zeta)$
    + Noisy-DQN loss:

![2018-01-31 17-29-15屏幕截图.png-14.4kB][2]

+ NoisyNet-A3C: similar to DQN
    + Entropy bonus of the policy loss is removed
    + FC layers of value function are parameterised as a noise network $\rightarrow$ processed per replay step
    + Independent Gaussian noise
    + As A3C uses n-step returns, optimisatin occurs every n steps, after each optimisation, the parameters of policy network are resampled

![2018-01-31 17-33-45屏幕截图.png-11.4kB][3]

## 3. Experiment
+ Task: 57 Atari games
+ Comparison: DRL with originam exploration methods ($\epsilon$-greedy and entropy bonus)
+ Evaluation:
    + Absolute performance : human normalised score
$$ 100 \times \frac{Score_{Agent} - Score_{Random}}{Score_{Human} - Score_{Random}}$$
    + Relative performance of NoisyNet agents to the respective baseline agent without noisy networks
$$ 100 \times \frac{Score_{NoisyNet} - Score_{Baseline}}{max(Score_{Human},Score_{Baseline}) - Score_{Random}}$$

![2018-01-31 21-33-33屏幕截图.png-91.8kB][4]
![2018-01-31 21-33-40屏幕截图.png-123.1kB][5]
![2018-01-31 21-33-50屏幕截图.png-111.1kB][6]
![2018-01-31 21-33-59屏幕截图.png-47.7kB][7]
![2018-01-31 21-34-09屏幕截图.png-311.1kB][8]

## 4. Summary
+ NoisyNet is a general method for exploration, which is easy to understand and implement
+ Can be applied to DQN (off-policy) and A3C (on-policy)
+ Surpass $\epsilon$-greedy and entropy bonus
+ **Have been integrated with other methods $\rightarrow$ Rainbow**
+ Further reading: make comparison with OpenAI's similar work

                


  [1]: http://static.zybuluo.com/VenturerXu/q0ypaqzv4cx8wbur4l8rda5w/2018-01-31%2015-55-45%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [2]: http://static.zybuluo.com/VenturerXu/9y073uxck3d9zbwkr0vdlxci/2018-01-31%2017-29-15%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [3]: http://static.zybuluo.com/VenturerXu/6oy75ge8kwt92t2co6zcat9l/2018-01-31%2017-33-45%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [4]: http://static.zybuluo.com/VenturerXu/zoaxvendskluaej2rvm6zizf/2018-01-31%2021-33-33%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [5]: http://static.zybuluo.com/VenturerXu/vppf6re5e52b5xcviq7zvh27/2018-01-31%2021-33-40%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [6]: http://static.zybuluo.com/VenturerXu/jbrow3tjs27j2jrydols4qzh/2018-01-31%2021-33-50%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [7]: http://static.zybuluo.com/VenturerXu/sd25u5aqggi2kgyw05rqzy7w/2018-01-31%2021-33-59%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [8]: http://static.zybuluo.com/VenturerXu/tifetjhaazki8qoy2hm7el5q/2018-01-31%2021-34-09%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png