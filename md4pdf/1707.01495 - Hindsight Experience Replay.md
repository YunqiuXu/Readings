# 1707.01495 - Hindsight Experience Replay

![2018-01-28 14-48-39屏幕截图.png-33kB][1]

+ **Yunqiu Xu**
+ Focus on sparse reward:
    + HER: allows sample-efficient learning from rewards
    + Reward can be sparse and binary
    + Do not need complicated reward engineering
    + Experiment on robot arm manipulating

+ Some references, implementations, and further readings
    + [DanielTakeshi's notes][2]
    + [Learning from mistakes with Hindsight Experience Replay][3]
    + [Two minute papers][4]
    + [OpenAI's implementation][5]
    + [minsangkim142's implementation][6]
    + OpenAI posted a further blog "[Ingredents for Robots Research][7]", which contains some possible improvements of HER
    + 1709.10089 - Overcoming Exploration in Reinforcement Learning with Demonstrations
    + 1712.00948 - Hierarchical Actor-Critic

---

## 1. Introduction
+ Challenges: reward engineering
    + Sparse reward is hard to deal
    + Designing reward function is complex
    + e-greedy based exploration is inefficient
+ Insight from human learning: 人类可以从不好的结果中吸取教训, 而RL只能根据得到的reward学习
+ 从另一份工作(Universal value function approximators, Schaul 2015)中得到的灵感: 每个episode都设定不同的目标 (**疑问, 是否类似课程学习这样循序渐进的**)

+ Hindsight Experience Replay: 
    + Suitable with off-policy RL (e.g. DQN)
    + Assumption: multiple goal can be achieved $\rightarrow$ 到达每个状态都会被给予不同的目标

## 2. Background
+ DDPG
    + AC-like DQN for continuous action spaces
    + Actor: 
        + $\pi : S \rightarrow A$
        + Target policy to choose action 
        + Try to maximize action value with respect to policy's parameters
    + Critic: 
        + $Q^{\pi} : S \times A \rightarrow R$
        + Action-value function to evaluate Q value 
        + Try to minimize Bellman error
    + Learning: update C using Bellman, update A using PG
+ UVFA: Universal Value Function Approximators
    + There are more than one goal we may try to achieve
    + Learning: for each episode sample a state-goal pair, so the "goal" stay fixed in this episode

## 3. HER

### 3. Problem Setup
+ Key idea: replay episodes with a different goal.
+ Assumption: need multiple goals in an environment, for each state, we can get specific reward


+ Store an episode $(s_1, s_2, ..., s_T)$ in replay buffer twice:
    + One is running with original goal
    + Another is with "final state" in this episode: if the agent still fails at $s_T$, then set this state $s_T$ as goal for this episode

+ Simplest version
    + Store both final state $s_T$ and original goal $g$ per episode
    + Shape a mapping function $m(s_T)$ to represent state-goal pair

![2018-03-03 16-27-13屏幕截图.png-112kB][8]

### 3.2 Algorithm

![2018-01-28 16-29-05屏幕截图.png-153.6kB][9]

### 3.3 Some Code Details
+ 第一个内循环用于构建 goals replay $G$
    + 正常跑算法 $A$ , 并把得到的transition $(s_t,a_t,r_t,s_{t+1},g)$ 存入 $G$
    + 注意这里 transition 和一般的 transition 有所不同, 多了一个 $g$
    + $g$ 即为任务预设的目标状态, 在这个循环里是不变的 `g = np.copy(env.target)`
+ 第二个内循环用于构建 experience replay $R$
    + 首先使用map function, 获取t时刻的reward $r_t$, 在具体实现过程中, 就直接从$G$里面**按顺序取出**一个transition进行改造, 即为初始 $g$ 下的transition
    ![2018-03-03 16-53-59屏幕截图.png-19.5kB][10]
    + 注意这里将 $s_t || g$ 表示为这两个状态的连接, 后同
    + 接下来在这里加入HER循环, 从$G$中**随机选取**一个transition, 将其 $s_{t+1}$ 作为该transition的goal, 存入 $R$
    ![2018-03-03 17-00-44屏幕截图.png-61.8kB][11]
+ 第三个内循环就是正常的 DQN, 从 $R$ 中选取minibatch进行学习, 注意需要对原有网络进行改造, 因为输入维度发生了变化
+ 可以看出HER并未对reward function进行改造, 只是用一些可能对学习有利的数据扩增了experience replay, 从而缓解sparse reward问题


## 4. Experiment

+ Robot arm manipulating tasks:
![2018-01-28 17-06-58屏幕截图.png-145kB][12]

+ Does HER improve performance
    + Multiple goals
![2018-01-28 17-08-06屏幕截图.png-121kB][13]
    + Only one goal
![2018-01-28 17-09-08屏幕截图.png-72.5kB][14]

+ How does HER interact with reward shaping (not only binary)
![2018-01-28 17-10-21屏幕截图.png-59.4kB][15]

+ How many goals should we replay each trajectory with and how to choose them
![2018-01-28 17-11-35屏幕截图.png-168.8kB][16]

## 5. Summary
+ HER:
    + Try to handle sparse reward
    + If the original goal can not be achieved in this episode, set final state as goal
    + Suitable for off-policy method: e.g. DQN / DDPG / Rainbow
+ Future work from [OpenAI's further blog][17]:

|Future work|Description|
|-----|
|Automatic hindsight goal creation| 现在的工作是比较简单但低效的做法, 即先跑一轮内循环构建goal replay, 然后再从中选取, 未来工作可尝试可学习的自动生成方法|
|Unbiased HER|-|
|**HER + HRL**|目前已有基于HER进行HRL的工作 (1712.00948 - Hierarchical Actor-Critic), 本工作设置了有层次的goal并应用HER, 未来工作可尝试应用HER于higher-level policy选择的动作. 一个例子, 我们之前的假定是在每个state够可以得到reward, 这里可以将假定修改为: 若高层指定低层实现目标A, 但底层终止于状态B, 这时可以假定高层指定底层实现目标B|
|Richer value functions| - |
|Faster information propagation|经典的构建target net的方法效率不高, 未来工作可以尝试其他稳定学习的方法|
|HER + multi-step returns|-|
|On-policy HER| 现在HER基于off-policy的算法, 可尝试on-policy, 如结合PPO. 相关方向工作 1711.06006 - Hindsight policy gradients|
| RL with very frequent actions| - |
| **HER与其他工作结合** | Rainbow, reverse curriculum learning, DQfD| 
    


  [1]: http://static.zybuluo.com/VenturerXu/xwtgozchnneq63soqbehybsf/2018-01-28%2014-48-39%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [2]: https://github.com/DanielTakeshi/Paper_Notes/blob/master/reinforcement_learning/Hindsight_Experience_Replay.md
  [3]: https://becominghuman.ai/learning-from-mistakes-with-hindsight-experience-replay-547fce2b3305
  [4]: https://www.youtube.com/watch?v=Dvd1jQe3pq0
  [5]: https://github.com/openai/baselines/tree/master/baselines/her
  [6]: https://github.com/minsangkim142/hindsight-experience-replay
  [7]: https://blog.openai.com/ingredients-for-robotics-research/
  [8]: http://static.zybuluo.com/VenturerXu/fmvh71autm58f2xnh7whcba2/2018-03-03%2016-27-13%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [9]: http://static.zybuluo.com/VenturerXu/fpmecyl1q4vk1h084pmngnvy/2018-01-28%2016-29-05%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [10]: http://static.zybuluo.com/VenturerXu/1o6nycxpfxf91326bdqa73gw/2018-03-03%2016-53-59%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [11]: http://static.zybuluo.com/VenturerXu/leecos0rg1bdw70hdsuqt7w2/2018-03-03%2017-00-44%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [12]: http://static.zybuluo.com/VenturerXu/3mrquspb461si5t6qti7llpr/2018-01-28%2017-06-58%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [13]: http://static.zybuluo.com/VenturerXu/of7hl1t6x9ut6cl15lk6uw7h/2018-01-28%2017-08-06%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [14]: http://static.zybuluo.com/VenturerXu/cr3kew7tixsbv6m7lhn7dm96/2018-01-28%2017-09-08%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [15]: http://static.zybuluo.com/VenturerXu/dlg5ou1vasvn9rxjo193fa3y/2018-01-28%2017-10-21%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [16]: http://static.zybuluo.com/VenturerXu/xzxsegbxv7emm0hqerrqunax/2018-01-28%2017-11-35%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [17]: https://blog.openai.com/ingredients-for-robotics-research/