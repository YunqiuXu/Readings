# 1707.01495 - Hindsight Experience Replay

![2018-01-28 14-48-39屏幕截图.png-33kB][1]

+ **Yunqiu Xu**
+ Focus on sparse reward:
    + HER: allows sample-efficient learning from rewards
    + Reward can be sparse and binary
    + Do not need complicated reward engineering
    + Experiment on robot arm manipulating

+ Some references, implementations, and further readings
    + [DanielTakeshi's notes][2]
    + [Learning from mistakes with Hindsight Experience Replay][3]
    + OpenAI posted a further blog "[Ingredents for Robots Research][4]", which contains some possible improvements of HER
    + [Paper Sharing one - Hindsight Experience Replay][5]
    + [事后诸葛亮，读Hindsight Experience Replay][6]
+ Implementations:
    + [OpenAI's implementation][7]
    + [minsangkim142's implementation][8]
+ Further readings:    
    + 1709.10089 - Overcoming Exploration in Reinforcement Learning with Demonstrations
    + 1712.00948 - Hierarchical Actor-Critic
    + 1711.06006 - Hindsight Policy Gradients

---

## 1. Introduction
+ Challenges: reward engineering
    + Sparse reward is hard to deal
    + Designing reward function is complex
    + e-greedy based exploration is inefficient
+ Insight from human learning: 人类可以从不好的结果中吸取教训, 而RL只能根据得到的reward学习
+ 从另一份工作(Universal value function approximators, Schaul 2015)中得到的灵感: 每个episode都设定不同的目标 (**疑问, 是否类似课程学习这样循序渐进的**)

+ Hindsight Experience Replay: 
    + Suitable with off-policy RL (e.g. DQN)
    + Assumption: 可以将每个state设置成goal, policy的目标为对于给定state, 输入相应action, 最终达到goal state
    + Policy 其实学到的是到达某个goal state的能力
    + 适合环境 : **goal-based with sparse rewards**

+ A simple example of HER

![2018-03-23 19-35-29屏幕截图.png-87.5kB][9]


    
## 2. Background
+ DDPG
    + AC-like DQN for continuous action spaces
    + Actor: 
        + $\pi : S \rightarrow A$
        + Target policy to choose action 
        + Try to maximize action value with respect to policy's parameters
    + Critic: 
        + $Q^{\pi} : S \times A \rightarrow R$
        + Action-value function to evaluate Q value 
        + Try to minimize Bellman error
    + Learning: update C using Bellman, update A using PG
+ UVFA: Universal Value Function Approximators
    + There are more than one goal we may try to achieve
    + Learning: for each episode sample a state-goal pair, so the "goal" stay fixed in this episode

## 3. HER

### 3.1 Problem Setup
+ Key idea: replay episodes with a different goal.
+ Assumption: 
    + Need multiple goals in an environment, for each state, we can get specific reward
    + 注意这里和sparse reward不冲突, 不更改reward function, 只是对于未完成的episode换了下final state, 然后计算reward的时候判断能否到达这个新的final state
    + 这里final state有可能和我们真正需要的目标很接近, 但也可能很远, 和课程学习或者HRL不同, 我们学习的并不是通过小/简单目标接近最终目标的能力, 而是给定目标到达目标的能力


+ Store an episode $(s_1, s_2, ..., s_T)$ in replay buffer twice:
    + One is running with original goal: $(s||g, a, r, s_{next}||g)$
    + Another is with "final state" in this episode: 
        + If the agent still fails at $s_T$, then set this state $s_T$ as goal for this episode
        + $(s||s_T, a, r_T, s_{next}||s_T)$
        + 注意这里$r_T$要重新计算, 因为目标状态改变了

+ Simplest version
    + Store both final state $s_T$ and original goal $g$ per episode
    + Shape a mapping function $m(s_T)$ to represent state-goal pair

![2018-03-03 16-27-13屏幕截图.png-112kB][10]

### 3.2 Algorithm

![2018-01-28 16-29-05屏幕截图.png-153.6kB][11]

### 3.3 Some Code Details
+ 第一个内循环用于构建 goals replay $G$
    + 正常跑算法 $A$ , 并把得到的transition $(s_t,a_t,r_t,s_{t+1},g)$ 存入 $G$
    + 注意这里 transition 和一般的 transition 有所不同, 多了一个 $g$
    + $g$ 即为任务预设的目标状态, 在这个循环里是不变的 `g = np.copy(env.target)`
+ 第二个内循环用于构建 experience replay $R$
    + 首先使用map function, 获取t时刻的reward $r_t$, 在具体实现过程中, 就直接从$G$里面**按顺序取出**一个transition进行改造, 即为初始 $g$ 下的transition
    ![2018-03-03 16-53-59屏幕截图.png-19.5kB][12]
    + 注意这里将 $s_t || g$ 表示为这两个状态的连接, 后同
    + 接下来在这里加入HER循环, 从$G$中**随机选取**一个transition, 将其 $s_{t+1}$ 作为该transition的goal, 存入 $R$. **注意这里因为goal state更换了, 计算 $r_n$ 时为计算是否到达了这个新目标, 到达了为0, 反之为-1**
    ![2018-03-03 17-00-44屏幕截图.png-61.8kB][13]
+ 第三个内循环就是正常的 DQN, 从 $R$ 中选取minibatch进行学习, 注意需要对原有网络进行改造, 因为输入维度发生了变化
+ 可以看出HER并未对reward function进行改造, 只是用一些可能对学习有利的数据扩增了experience replay, 从而缓解sparse reward问题


## 4. Experiment

+ Robot arm manipulating tasks in this work
![2018-01-28 17-06-58屏幕截图.png-145kB][14]

+ Does HER improve performance
    + Multiple goals
![2018-01-28 17-08-06屏幕截图.png-121kB][15]
    + Only one goal
![2018-01-28 17-09-08屏幕截图.png-72.5kB][16]

+ How does HER interact with reward shaping (not only binary)
![2018-01-28 17-10-21屏幕截图.png-59.4kB][17]

+ How many goals should we replay each trajectory with and how to choose them
![2018-01-28 17-11-35屏幕截图.png-168.8kB][18]

+ [OpenAI's further blog][19]中又添加了一些新的基于目标的环境
    + 注意这个是 goal-based: agent有固定目标, **存疑: 能否应用到atari游戏?**
    + [FetchSlide-v0][20]: 击打黑色冰球到红色位置
    ![fetch-slide.png-393.1kB][21]
    + [HandManipulateBlock-v0][22]: 将方块旋转至指定方向
    ![hand-block.png-685.1kB][23]
    + Result : 貌似对于非sparse reward结果不怎么样呀
    ![2018-03-23 16-33-37屏幕截图.png-84.2kB][24]


## 5. Summary
+ HER:
    + Works well for goal-based environment with sparse reward
    + If the original goal can not be achieved in this episode, set final state as goal
    + Suitable for off-policy method: e.g. DQN / DDPG / Rainbow
+ Future work from [OpenAI's further blog][25]:

|Future work|Description|
|-----|
|Automatic hindsight goal creation| 现在的工作是比较简单但低效的做法, 即先跑一轮内循环构建goal replay, 然后再从中选取, 未来工作可尝试可学习的自动生成方法|
|**HER + HRL**|目前已有基于HER进行HRL的工作 (1712.00948 - Hierarchical Actor-Critic), 本工作设置了有层次的goal并应用HER, 未来工作可尝试应用HER于higher-level policy选择的动作. 一个例子, 我们之前的假定是在每个state够可以得到reward, 这里可以将假定修改为: 若高层指定低层实现目标A, 但底层终止于状态B, 这时可以假定高层指定底层实现目标B|
|Faster information propagation|经典的构建target net的方法效率不高, 未来工作可以尝试其他稳定学习的方法|
|**On-policy HER**| 现在HER基于off-policy的算法, 可尝试on-policy, 如结合PPO. 相关方向工作 1711.06006 - Hindsight policy gradients|
| **HER与其他工作结合** | Rainbow, reverse curriculum learning, DQfD| 
|Unbiased HER|-|
|Richer value functions| - |
|HER + multi-step returns|-|
| RL with very frequent actions| - |
    


  [1]: http://static.zybuluo.com/VenturerXu/xwtgozchnneq63soqbehybsf/2018-01-28%2014-48-39%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [2]: https://github.com/DanielTakeshi/Paper_Notes/blob/master/reinforcement_learning/Hindsight_Experience_Replay.md
  [3]: https://becominghuman.ai/learning-from-mistakes-with-hindsight-experience-replay-547fce2b3305
  [4]: https://blog.openai.com/ingredients-for-robotics-research/
  [5]: https://zhuanlan.zhihu.com/p/34842248
  [6]: https://zhuanlan.zhihu.com/p/34309324
  [7]: https://github.com/openai/baselines/tree/master/baselines/her
  [8]: https://github.com/minsangkim142/hindsight-experience-replay
  [9]: http://static.zybuluo.com/VenturerXu/uif1m6ffjvh8lvawfe2cs47s/2018-03-23%2019-35-29%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [10]: http://static.zybuluo.com/VenturerXu/fmvh71autm58f2xnh7whcba2/2018-03-03%2016-27-13%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [11]: http://static.zybuluo.com/VenturerXu/fpmecyl1q4vk1h084pmngnvy/2018-01-28%2016-29-05%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [12]: http://static.zybuluo.com/VenturerXu/1o6nycxpfxf91326bdqa73gw/2018-03-03%2016-53-59%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [13]: http://static.zybuluo.com/VenturerXu/leecos0rg1bdw70hdsuqt7w2/2018-03-03%2017-00-44%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [14]: http://static.zybuluo.com/VenturerXu/3mrquspb461si5t6qti7llpr/2018-01-28%2017-06-58%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [15]: http://static.zybuluo.com/VenturerXu/of7hl1t6x9ut6cl15lk6uw7h/2018-01-28%2017-08-06%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [16]: http://static.zybuluo.com/VenturerXu/cr3kew7tixsbv6m7lhn7dm96/2018-01-28%2017-09-08%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [17]: http://static.zybuluo.com/VenturerXu/dlg5ou1vasvn9rxjo193fa3y/2018-01-28%2017-10-21%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [18]: http://static.zybuluo.com/VenturerXu/xzxsegbxv7emm0hqerrqunax/2018-01-28%2017-11-35%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [19]: https://blog.openai.com/ingredients-for-robotics-research/
  [20]: https://gym.openai.com/envs/FetchSlide-v0/
  [21]: http://static.zybuluo.com/VenturerXu/3x16jpk1gdxzzn6v9yf1fr5c/fetch-slide.png
  [22]: https://gym.openai.com/envs/HandManipulateBlock-v0/
  [23]: http://static.zybuluo.com/VenturerXu/kbqfy22tuaeukhiuismzzvk1/hand-block.png
  [24]: http://static.zybuluo.com/VenturerXu/1i49sh4scm25zm4i1fpi4jpz/2018-03-23%2016-33-37%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [25 https://blog.openai.com/ingredients-for-robotics-research/