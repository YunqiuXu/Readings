# 1707.01495 - Hindsight Experience Replay

![2018-01-28 14-48-39屏幕截图.png-33kB][1]

+ **Yunqiu Xu**
+ Focus on sparse reward:
    + HER: allows sample-efficient learning from rewards
    + Reward can be sparse and binary
    + Do not need complicated reward engineering
    + Experiment on robot arm manipulating

+ I treat this work as further reading for imitation learning, following are some reference:
    + [Another's notes][2]
    + [Two minute papers][3]
    
---

## 1. Introduction
+ Challenges: reward engineering $\rightarrow$ sparse reward
+ Insight from human learning: 
    + 人类从undesired outcome中学习到同desired ouecome中一样多的信息
    + 换言之, 人类可以从不好的结果中吸取教训, 而RL只能根据得到的reward学习
+ 从另一份工作(Universal value function approximators, Schaul 2015)中得到的灵感: 每个episode都设定不同的目标 (疑问, 是否类似课程学习这样循序渐进的)
+ Hindsight Experience Replay: 
    + Suitable with off-policy RL (e.g. DQN)
    + Assumption: multiple goal can be achieved $\rightarrow$ 到达每个状态都会被给予不同的目标

## 2. Background
+ DDPG
    + AC-like DQN for continuous action spaces
    + Actor: 
        + $\pi : S \rightarrow A$
        + Target policy to choose action 
        + Try to maximize action value with respect to policy's parameters
    + Critic: 
        + $Q^{\pi} : S \times A \rightarrow R$
        + Action-value function to evaluate Q value 
        + Try to minimize Bellman error
    + Learning: update C using Bellman, update A using PG
+ UVFA: Universal Value Function Approximators
    + There are more than one goal we may try to achieve
    + Learning: for each episode sample a state-goal pair, so the "goal" stay fixed in this episode

## 3. HER
+ Key idea: replay episodes with a different goal.
+ Assumption: need multiple goals in an environment.
+ HER can be combined with off-policy RL algorithms, so it doesn't replace them but can augment them
+ Store an episode $(s_1, s_2, ..., s_T)$ in replay buffer twice:
    + One is with original goal
    + Another it with "final goal" in this episode: if the agent still fails at $s_T$, then set $s_T$ as goal for this episode
+ Simplest version
    + Store both final state $s_T$ and original goal $g$ per episode
    + Shape a mapping function $m(s_T)$ to represent state-goal pair


![2018-01-28 16-29-05屏幕截图.png-153.6kB][4]

## 4. Experiment

+ Robot arm manipulating tasks:
![2018-01-28 17-06-58屏幕截图.png-145kB][5]

+ Does HER improve performance
    + Multiple goals
![2018-01-28 17-08-06屏幕截图.png-121kB][6]
    + Only one goal
![2018-01-28 17-09-08屏幕截图.png-72.5kB][7]

+ How does HER interact with reward shaping (not only binary)
![2018-01-28 17-10-21屏幕截图.png-59.4kB][8]

+ How many goals should we replay each trajectory with and how to choose them
![2018-01-28 17-11-35屏幕截图.png-168.8kB][9]

## 5. Summary
+ Try to handle sparse reward
+ If the original goal can not be achieved in this episode, set final state as goal
+ An implementation of HER in imitation learning: 1709.10089 - Overcoming Exploration in Reinforcement Learning with Demonstrations

  [1]: http://static.zybuluo.com/VenturerXu/xwtgozchnneq63soqbehybsf/2018-01-28%2014-48-39%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [2]: https://github.com/DanielTakeshi/Paper_Notes/blob/master/reinforcement_learning/Hindsight_Experience_Replay.md
  [3]: https://www.youtube.com/watch?v=Dvd1jQe3pq0
  [4]: http://static.zybuluo.com/VenturerXu/fpmecyl1q4vk1h084pmngnvy/2018-01-28%2016-29-05%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [5]: http://static.zybuluo.com/VenturerXu/3mrquspb461si5t6qti7llpr/2018-01-28%2017-06-58%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [6]: http://static.zybuluo.com/VenturerXu/of7hl1t6x9ut6cl15lk6uw7h/2018-01-28%2017-08-06%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [7]: http://static.zybuluo.com/VenturerXu/cr3kew7tixsbv6m7lhn7dm96/2018-01-28%2017-09-08%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [8]: http://static.zybuluo.com/VenturerXu/dlg5ou1vasvn9rxjo193fa3y/2018-01-28%2017-10-21%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [9]: http://static.zybuluo.com/VenturerXu/xzxsegbxv7emm0hqerrqunax/2018-01-28%2017-11-35%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png