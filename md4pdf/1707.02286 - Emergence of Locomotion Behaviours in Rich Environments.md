# 1707.02286 - Emergence of Locomotion Behaviours in Rich Environments


+ **Author: Yunqiu Xu**
+ Similar work from OpenAI: [1707.06347 - Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)
+ PPO have some of the benefits of TRPO, but much simpler to implement, more general, and have better sample complexity (empirically)

+ Other reference:
    + https://zhuanlan.zhihu.com/p/30138538
    + https://www.leiphone.com/news/201707/A2TWlxblaBFl8aod.html
    + https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/6-4-DPPO/

-----

+ Previous methods:
    + Q-learning with function approximation: fails on many simple problems and is poorly understood
    + Vanilla policy gradient: poor data effiency and robustness
        + Sparse reward: step size is prone to be too large
        + Get target by sampling: maybe the target is not the best(local optima)
    + TRPO: 
        + Constraint: the KL divergence of old policy and new policy should not exceed threshold
        + Complicated, not compatible with architectures that include noise or parameter sharing


+ PPO from OpenAI: 
    + Improve the current state of affairs by introducing an algorithm that attains the data efficiency and reliable performance of TRPO, while using only first-order optimization
    + PPO put constraint into loss function: 
        + If new policy is in wrong opt direction : still need to optimize
        + If new policy is in right opt direction but with too large learning rate (much different from old policy) : stop optimizing
        + If new value is worse than old (difference from target): still need to optimize
        + If new value is better than old but go too far (much different from old): stop optimizing


+ PPO from DeepMind: Distributed PPO
    ![DPPO_DEEPMIND.png-122.6kB][1]
    + Estimate Advantage: 
        + $sum_{t' \gt t}\gamma^{t' - t}r_{t'}$ : expected future return approximated with a sample rollout
        + $V_{\psi}(s_t)$ : learned approximation with parameters $\psi$
    + Similar to AC: Actor try to maximize $J_{PPO}$, critic try to minimize $L_{BL}$
    + Scaling term $\alpha \gt 1$ : 
        + If new policy is much different from old $\rightarrow$ similar to large learning rate $\rightarrow$ hard to converge
        + If KL-divergence significantly different from the target KL (we do not want to see this) , increase its importance in $J_{PPO}$
        + Controls the adjustment of the KL-regularization coefficient


    
    
+ Some details for DPPO:
    + D: sets a threshold for the number of workers whose gradients must be available to update the parameters
    + M, B: the number of sub-iterations with policy and baseline updates
given a batch of datapoints.
    + T: the number of data points collected per worker before parameter
updates are computed
    + K(for RNNs): the number of time steps for computing K-step returns and truncated backprop through time
![DPPO_detail1.png-52.9kB][2]
![DPPO_detail2.png-149.5kB][3]
    


  [1]: http://static.zybuluo.com/VenturerXu/ij5a3zx74mj5why219aao3nk/DPPO_DEEPMIND.png
  [2]: http://static.zybuluo.com/VenturerXu/8ewjdfv4floapmy0pknun62p/DPPO_detail1.png
  [3]: http://static.zybuluo.com/VenturerXu/3pabciwdg4rg6akun1110mfw/DPPO_detail2.png