# 1707.02747 - Robust Imitation of Diverse Behaviors

+ **Yunqiu Xu**
+ Other reference:
    + [最前沿：机器人学习Robot Learning之模仿学习Imitation Learning的发展][1]
    + [DeepMind发表物理智能最新研究：如何在仿真环境中生成灵活行为][2]
    + [Robust Imitation of Diverse Behaviors][3]


+ I think this is similar to DART: broaden the demonstration via noise injection $\rightarrow$ make learning more robust

## 1. Introduction
+ Challenge for imitation learning:
    + Supervised learning, VAE (behavior cloning): 
        + Can model diverse behaviors without dropping modes
        + Not robust, hard to handle agent trajectory diverges from the demonstrations
        + Need large training datasets for non-trival tasks
    + Generative adversarial imiataion learning
        + Can learn more robust policies with fewer demonstrations
        + More difficult to train: oscilliating / model collapse

+ Our work
    + Combine SL and GAIL
    + SL: new VAE for supervised imitation $\rightarrow$ learn semantic policy embeddings
    + GAIL
        + More robust than supervised learning
        + Avoid model collapse

+ The model learns, from a moderate number of demonstration trajectories 
    + A semantically well structured embedding of behaviors
    + A corresponding multi-task controller that allows to robustly execute diverse behaviors from this embedding space
    + An encoder that can map new trajectories into the embedding space and hence allows for one-shot imitation.

## 2. A Generative Modeling Approach to Imitating Diverse Behaviors

+ Behavior cloning with VAE
![VAE001.png-116.3kB][4]
    + Try to minimize
    ![VAE002.png-16.7kB][5]

+ Diverse generative adversarial imitation learning
    + Enable GAIL to produce diverse solutions
    + Discriminator:
    ![GAIL001.png-16.2kB][6]
    + Value function
    ![GAIL002.png-12.4kB][7]
    + Pseudo code
    ![GAIL003.png-71.9kB][8]

## 3. Experiments
+ Robotic arm reaching
![EXP001.png-103.7kB][9]
+ 2D Walker
![EXP002.png-363.2kB][10]
+ Complex humanoid
![EXP003.png-282.1kB][11]

+ The evaluation is based on the diversity of policies, not game score
+ GAIL policies are more robust than those of VAE policies


## 4. Conclusion
+ Combine the strength of some generative models: VAE + GAIL
+ Note that VAE is a method for supervised imitation (BC)


  [1]: https://zhuanlan.zhihu.com/p/27935902
  [2]: https://zhuanlan.zhihu.com/p/27815884
  [3]: https://github.com/DanielTakeshi/Paper_Notes/blob/master/reinforcement_learning/Robust_Imitation_of_Diverse_Behaviors.md
  [4]: http://static.zybuluo.com/VenturerXu/fd1jvyg3pon6fwcts4p4mfd0/VAE001.png
  [5]: http://static.zybuluo.com/VenturerXu/ozcj987tws58e4oitft9i67k/VAE002.png
  [6]: http://static.zybuluo.com/VenturerXu/ed0fugrgfhuzzfro86owgf79/GAIL001.png
  [7]: http://static.zybuluo.com/VenturerXu/la2p18kmh169bvlkmabyuml3/GAIL002.png
  [8]: http://static.zybuluo.com/VenturerXu/r42nec6uaulmgdmpgwoat87r/GAIL003.png
  [9]: http://static.zybuluo.com/VenturerXu/b3s8hvb7gylvgblgyhk7qzk8/EXP001.png
  [10]: http://static.zybuluo.com/VenturerXu/romlyh2brx6qmsh0fnkyg7mi/EXP002.png
  [11]: http://static.zybuluo.com/VenturerXu/xd59x4y4xz1ex08b2uumsaor/EXP003.png