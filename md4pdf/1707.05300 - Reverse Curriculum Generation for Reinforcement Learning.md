# 1707.05300 - Reverse Curriculum Generation for Reinforcement Learning

+ **Yunqiu Xu**
+ 在[Deep RL Bootcamp][1]上看到的一篇文章, 为了解决sparse reward问题, 使用反向课程学习, 从目标开始倒推

---

## 1. Introduction
+ Challenging: 
    + Sparse reward $\rightarrow$ hard for learning-based approaches and non-sparse reward functions
    + The use of demonstrations $\rightarrow$ requires an expert intervention
+ Key insights:
    + It's easier to reach the goal from states nearby the goal
    + Applying random actions from such states makes the agent go to new feasible nearby states, thus easier to reach the goal

+ Our method:
    + **Do not use reward engineering and demonstrations**
    + Requires no prior knowledge of the task, only need to provide the final state (target position)
    + Train the robot to reach the goal which the start position is nearby the goal
    + Then train it from further start position
    + How to choose start position: 
        + Perform random walk from previous start states
        + You can get reward by starting from these states: can reach final state
        + But these are not best start states: require more training


## 2. Related Work on Curriculum Learning

+ Reject examples which is too hard currently: 
    + Applied in SL and RL with pre-specified task sequences
    + Few implementations, only preliminary tasks
+ Intrinsic motivation based on learning progress:
    + Obtain "developmental trajectories"
    + Requires iteratively partitioning the full task space
+ Use baseline performance of easy tasks to gauge hard tasks
    + Can only handle finite sets of tasks
    + Requires each task to be learnable on its own
+ Our method:
    + Train a policy that can generalize to continuousl parameterized tasks
    + Perform well under sparse rewards, do not allocate training effore to tasks

## 3. Problem Definition
+ Learn a policy that leads a system into a specified goal space, from any start state sampled from a given distribution.
    + A large set of start states $S^0$ : more robust than using only one start state, avoid undesired deviations from intended trajectory
    + A small set of goals $S^g$ : goal, as well as its nearby states

![2017-11-30 15-41-50屏幕截图.png-9.8kB][2]

+ Assumptions for reverse curriculum generation
    + We can arbitrarily reset the agent into any start state $s^0 \in S$ at the beginning of all trajectories.
    + $S^g$ is not empty $\rightarrow$ at least one goal state
    + The Markov Chain induced by taking uniformly sampled random actions has a communicating class including all start states $S^0$ and the given goal state $s^g$

## 4. Methodology

![2017-11-30 15-48-11屏幕截图.png-198.3kB][3]

## 5. Experimental Results
+ Questions:
    + Does the performance of the policy on the target start state distribution $\rho_0$ improve by training on distributions $\rho_i$ growing from the goal?
    + Does focusing the training on “good starts” speed up learning?
    + Is Brownian motion a good way to generate “good starts” from previous “good starts”?

+ Tasks: http://bit.ly/reversecurriculum
    + Point-mass maze
    + Ant maze
    + Ring on Peg task
    + Key insertion task

+ Results:
![2017-11-30 16-01-04屏幕截图.png-513.8kB][4]
![2017-11-30 16-02-11屏幕截图.png-245.9kB][5]

## 6. Conclusion and Future Work

+ Conclusion: 
    + Propose a method to automatically adapt the start state distribution on which an agent is trained
    + If 3 assumptions are satisfied, hard goal-oriented problems can be tackled
+ Future work: 
    + Combine curriculum-generation with goal generation ([1705.06366 - Automatic goal generation for reinforcement
learning agents][6])
    + Combine curriculum-generation with domain randomization ([1703.06907 - Domain Randomization
for Transferring Deep Neural Networks from Simulation to the Real World][7]) $\rightarrow$ policy can be transferred to real world
                


  [1]: https://sites.google.com/view/deep-rl-bootcamp/lectures
  [2]: http://static.zybuluo.com/VenturerXu/z77jw33m4ej7h6od44kascnf/2017-11-30%2015-41-50%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [3]: http://static.zybuluo.com/VenturerXu/3cl59e47vnar2w6xqq203mys/2017-11-30%2015-48-11%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [4]: http://static.zybuluo.com/VenturerXu/ecpzkvk2lx2mh6xm5zdatq4q/2017-11-30%2016-01-04%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [5]: http://static.zybuluo.com/VenturerXu/0whyi3b4hrlyqqc2qb5gj2mz/2017-11-30%2016-02-11%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [6]: https://arxiv.org/abs/1705.06366
  [7]: https://arxiv.org/abs/1703.06907