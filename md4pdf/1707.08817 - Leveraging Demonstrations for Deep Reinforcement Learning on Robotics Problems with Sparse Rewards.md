# 1707.08817 - Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards

![2018-01-28 19-46-22屏幕截图.png-66.7kB][1]

+ **Yunqiu Xu**
+ DDPGfD:
    + Combine DQfD with DDPG $\rightarrow$ continuous action space
    + Typical RL: carefully engineered shaping rewards $\rightarrow$ sample inefficiency
    + Use demonstration: reduce exploration
    + Similar to DQfD, store both demonstration and actual interactions in replay buffer, and use prioritized mechanism
    + Experiment on both simulated and real robot manipulating tasks

---

## 1. Introduction
+ Challenges: robot task with sparse reward
    + The goal of hand-coded reward function is natural to be sparse
    + Use demonstration can reduce exploration and carefully reward shaping

+ This work can be seen as an extention of DQfD
+ Why use DDPG:
    + Compared with on-policy methods, it can learn from arbitrary transition data
    + Experience replay $\rightarrow$ maintain these transitions to propagate sparse rewards
    + Compared with vanilla DQN, DDPG can handle continuous action space
    

## 2. Related Work
+ Our work: combine imitation learning with learning from task rewards, to handle continuous action space

+ DAGGER:
    + Simple imitation learning, can not learn to improve
    + Need human expert's participation
+ Inverse RL
    + Learn cost/reward function under which the demonstration is optimal
    + Requires knowledge of the dynamics
    + Can not handle continuous tasks

+ Guided Cost Learning (GCL) and GAIL:
    + Do not need knowledge of dynamics and hand-crafted features
    + Learn reward and policy to imitate expert demonstrations
    + At each episode, sample demonstratons of current policy, then use these sampled demonstrations and expert demonstration to generate reward function
    + GCL and GAIL are different from how they generate reward function
    + Weakness:
        + Data generation can be unstable (e.g. GAN)
        + Can not handle consinuous tasks
+ Guided Policy Search
    + Find an optimal policy by decomposing the task into 3 steps
    + Step 1: learn dynamics from expert demonstrations
    + Step 2: Find locally optimal policies via optimal contral methods (e.g. iLQG or DDP)
    + Step 3: Fit the demonstrations via supervised learning
    + Assumption: reward must be shaped carefully, which may be impractical
+ DQfD $\rightarrow$ discrete version of this work

## 3. DDPG from Demonstrations
+ Modification 0: Before training, store expert demonstrations into replay buffer, and keep them forever
+ Modification 1: use pritorized replay to handle sparse rewards
$$p_i = \delta_i^2 + \lambda_3|\nabla_aQ(s_i,a_i|\theta^Q)|^2 + \epsilon + \epsilon_D$$
    + $\delta_i$ : TD error for this transition
    + $|\nabla_aQ(s_i,a_i|\theta^Q)|^2$ : loss applicd to the actor
    + $\epsilon$ and $\epsilon_D$ : probabilities for all / expert demonstrations to be sampled
+ Modification 2: mix 1-step and n-step returns $\rightarrow$ help to propagate the Q-values along the trajectories
+ Modification 3: do multiple learning updates per env step
    + With same minibatch, multiple updates will be performed to improve data efficiency
    + May lead to unstable $\rightarrow$ 20-40 in practice $\rightarrow$ balance efficiency and stability
+ Modification 4: L2 regularization to stabilize final performance

+ DDPG: 
    + Actor: policy to maximize action value with respect to parameters, update by policy gradient
    + Critic: action-value function to evaluate Q value, update by Bellman function
$$\nabla_{\theta^{\pi}}L_{Actor}(\theta^{\pi}) = -\nabla_{\theta^{\pi}}J(\theta^{pi}) + \lambda_2\nabla_{\theta^{\pi}}L_{reg}^A(\theta^{\pi})$$
$$L_{Critic}(\theta^Q) = L_1(\theta^Q) + \lambda_1L_n(\theta^Q) + \lambda_2L_{reg}^C(\theta^Q)$$

+ Summary of modifications compared with DDPG
    + Add human demonstrations into replay buffer
    + Pritorized replay
    + Mix 1-step ($L_1(\theta^Q)$) and n-step return ($L_n(\theta^Q)$)
    + Learning multiple times per env step $\rightarrow$ **DQfD 未提及**
    + L2 regularization

## 4. Experiment
+ Tasks: easy to specify goal, but difficult to specify distance function for reward shaping $\rightarrow$ easy to stuck in local optimal
    + A set of insertion tasks with a range of exploration difficulties
    ![2018-01-28 21-18-05屏幕截图.png-137.8kB][2]
    + 2 kinds of reward functions
    + Devide: 7-DOF robot arm
    + Demonstration data is collected by human demonstrator

+ Result 1: performance
![2018-01-28 21-24-14屏幕截图.png-257.4kB][3]
        
+ Result 2: the number of demonstration
+ Result 3: real robot
![2018-01-28 21-26-27屏幕截图.png-190.8kB][4]

## 5. Summary
+ Extend DQfD to DDPGfD $\rightarrow$ continuous action space
+ One-shot imitation can be achieved for some injection tasks
+ This work can be compared with OPENAI's work : 1709.10089 - Overcoming Exploration in Reinforcement Learning with Demonstrations


  [1]: http://static.zybuluo.com/VenturerXu/8o251kx6qag85t1zm4mmjqrp/2018-01-28%2019-46-22%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [2]: http://static.zybuluo.com/VenturerXu/5lu9v9aat9lxh2cqhdh8goeq/2018-01-28%2021-18-05%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [3]: http://static.zybuluo.com/VenturerXu/2i9bir2ptf3qlalabglx3zsa/2018-01-28%2021-24-14%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [4]: http://static.zybuluo.com/VenturerXu/awguk6c5ynq9hzc9f8niljn1/2018-01-28%2021-26-27%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png