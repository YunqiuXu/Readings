# 1709.04579 - Autonomous Extracting a Hierarchical Structure of Tasks in Reinforcement Learning and Multi-task Reinforcement Learning

+ **Yunqiu Xu**

-----

## 1. Introduction

+ Challenges: Curse of dimensionality $\rightarrow$ Slow learning speed

+ Recent related work includes FeUdal Net and Option Framework, similar to how they are mentioned in [Meta Learning Shared Hierarchies](https://arxiv.org/abs/1710.09767) $\rightarrow$ hard to handle MTRL

+ Our goal is to speed up learning in both single task RL and multi-task RL

+ Our work: 
    + ARM-HSTRL $\rightarrow$ use association rule to extract sub-goals and their relationships, thus autonomously decompose tasks as hierarchical structure
    + Our method does not need the action model in advance or a separate phase of learning to obtain the required data for extracting hierarchy.
    + Our method does not need the action model in advance or a separate phase of learning to obtain the required data for extracting hierarchy.


## 2. ARM-HSTRL

+ ARM: extracts association rules
    + Generate frequent itemsets using FP-growth
    + Generate association rules

![001.png-39.1kB][1]

+ HST-construction: converts association rules to a hierarchical structure tree

![002.png-98.7kB][2]

+ ARM-HSTRL in MTRL
    + Some details can be found at [Taylor and Stone (2009) Transfer learning for reinforcement learning domains: A survey](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.148.7615&rep=rep1&type=pdf)

## 3. Experiment
+ The experiments show the performance of Q-learning and ARM-HSTRL, from the figures we can see the significant difference, in both single task and MTRL


## 4. Conclusion
+ Use association rule mining to extract sub-goals and build hierarchical structure
+ Do not need the action model
+ Do not limited to factored MDP
+ Can learn from different and several trajectories
+ Do not need to clean and to process the paths
+ Efficient, practical, and leads to hierarchical optimal policies
+ ARM-HSTRL can handle MTRL and transfer its learning among tasks with different transition functions, while a lack of structural knowledge makes Q-learning impractical.

    


  [1]: http://static.zybuluo.com/VenturerXu/vzfy687rqwd688xq9ooyldsb/001.png
  [2]: http://static.zybuluo.com/VenturerXu/dbpv9xd7abb1vdxiuxold90i/002.png