# 1709.04905 - One-Shot Visual Imitation Learning via Meta-Learning

+ **Yunqiu Xu**
+ Other reference:
    + Presentation: https://www.youtube.com/watch?v=_9Ny2ghjwuY
    + Code: https://github.com/tianheyu927/mil

---

## 1. Introduction
+ Challenge: learning each skill from a scratch is infeasible
+ One-Shot Visual Imitation Learning via Meta-Learning
    + Reuse past experience to train the "base model", then adapt it to new task with only a single demonstration
    + Visual: use raw visual inputs
    + Meta-Learning: MAML [C. Finn et.al. 2017](https://arxiv.org/abs/1703.03400)

+ Prior work : take task identity / demonstration as the input into a contextual policy
+ Our work : learn parameterized policy, then adapt into new tasks through a few gradient updates


## 2. Related work
+ In this work, the state of environment is unknown $\rightarrow$ we feed raw sensory inputs to learn it
+ Two challenges for learning from demonstrations then applying it to real world:
    + Compounding errors: not in this work
    + **The need of a large number of demonstrations for each task**

+ Why don't use Inverse RL:
    + How does it work : recover reward function from demonstrations
    + Pros: reduce demonstrations, better than behavioral cloning
    + Cons: 
        + Requires additional robot experience to optimize the reward  [C. Finn et.al. 2016](https://arxiv.org/abs/1603.00448)
        + Hard to evaluate learned reward, especially for high-dim data (image)
        + Gan-based IRL (e.g. GAIL) is hard to train 
+ How do we reduce the demonstrations: **share data across tasks**
    + First, use a dataset of demonstrations of many other tasks for meta learning, in this way we can build a base model
    + Then we can adapt this base model to new task with only a few demonstrations
    + Meta-learning is similar to transfer learning to some extent, the different is not the transfer on dataset, but the transfer on task
    + Take a simple instance, if the robot is learned to pick apple, orange, pear ... then it can pick peach easily


## 3. Meta-Imitation Learning
+ Goal : learn a policy that can quickly adapt to new tasks from a single demonstration of that task

+ Each imitation task $T_i = \left\{\tau = \{o_1,a_1,...,o_T,a_T\} \sim \pi_i^*, L(a_{1:T}, \hat{a}_{1:T}), T \right\}$
    + $\tau$ : a demonstration generated by policy $\pi_i^*$
    + $L(a_1,...,a_T, \hat{a}_1, ..., \hat{a}_T) \rightarrow R$ : loss function to give feedback
    + **Note that this form is different from original MAML**

### 3.1 MAML
+ Consider a policy $\pi$ with parameter vector $\theta$
+ Sample a task $T_i$ from $p(T)$
+ Train this task with $K$ samples (adapt $\pi$ to $T_i$ to get new parameter $\theta'$)
+ Test this task, then treat the testing error as the training error of meta-process (Use $\theta_1', ..., \theta_n',$ to update $\theta$)
+ Meta objective:
    $$\min_{\theta} \sum_{T_i \sim p(T)} L_{T_i}(f_{\theta_i'}) = \sum_{T_i \sim p(T)}L_{T_i}(f_{\theta - \alpha \nabla_{\theta}L_{T_i}(f_{\theta})}) \space\space\space\space\space\space(1)$$
+ Finally, you can adapt trained $\pi$ to a new task with only a few data / gradient updates
![2017-11-27 00-11-57屏幕截图.png-470.7kB][1]

### 3.2 Extend MAML to MIL
+ $o_t$ is the observation at time $t$, i.e. an image, while $a_t$ is the action
+ For demonstration trajectory $\tau$, we use MSE to compute loss:
$$L_{T_i}(f_{\phi}) = \sum_{\tau_j \sim T_i}\sum_t||f_{\phi}(o_t^{(j)}) - a_t^{(j)}||_2^2 \space\space\space\space\space\space(2)$$
+ During meta-learning, we assume each task has at least 2 demonstrations, thus we can sample a set of tasks with two demonstrations per task
+ Compute $\theta_i'$ with one demonstration $\rightarrow$ inner loop of meta-learning
+ Use another demonstration to "test" $\theta_i'$ $\rightarrow$ update $\theta$ according to the gradient of meta-objective 
+ Meta-testing:
    + Sample a new task T and its one demonstration
    + This task can involve new goals or manipulating new, previously unseen objects.
    + Then we can adapt $\theta$ to this task

![2017-12-08 21-56-34屏幕截图.png-130.1kB][2]

### 3.3 Two Head Structure
+ Why use this: more flexibility during adapting
+ The parameters of pre-update head are not used for post-update head in final
+ Modification : parameters of final layers are not shared, forming two heads
    + Change loss function as:
    $$L_{T_i}(f_{\phi}) = \sum_{\tau_j \sim T_i}\sum_t||Wy_t^{(j)} + b - a_t^{(j)}||_2^2 \space\space\space\space\space\space(3)$$
        + $y_t^{(j)}$ : post-synamptic activations of the last hidden layer
        + $W,b$ : weights and bias for last layer
    + Then the meta-objective is about $\theta, W,b$
    $$\min_{\theta, W, b} \sum_{T_i \sim p(T)} L_{T_i}(f_{\theta_i'}) = \sum_{T_i \sim p(T)}L_{T_i}(f_{\theta - \alpha \nabla_{\theta}L_{T_i}(f_{\theta})}) \space\space\space\space\space\space(4)$$

### 3.4 Learning to Imitate without Expert Actions
+ Why use this : it is more practical to simply provide a video of the task being performed
+ We just simplify this problem by simplify the loss function as
$$L_{T_i}(f_{\phi}) = \sum_{\tau_j \sim T_i}\sum_t||Wy_t^{(j)} + b ||_2^2 \space\space\space\space\space\space(3)$$
+ **This can be a future question for more robust loss function**

## 4. Network Architecture

![2017-12-08 22-27-01屏幕截图.png-190.3kB][3]

+ **Layer normalization** after each layer
    + Data within a demonstration trajectory is highly correlated across time
    + Thus BN was not effective
+ Bias transformation $\rightarrow$ improve the performance of meta-learning
    + Concatenate a vector of parameters to a hidden layer of post-synaptic activations
    + Thus vector is treated as same as other parameters during meta-learning and final testing
    $$y = Wx+b \space\space \rightarrow \space\space y = W_1x + W_2z + b$$
        + $z$ is the parameter vector form bias transformation
        + $W = [W_1, W_2]$
    + This modification **increases the representational power of the gradient**
    + Does not affect the representation power of the network itself


## 4. Experiment

+ Questions:
    + Can a policy be learned to maps from image pixels to actions using a single demonstration of a task
    + How does our meta-imitation learning method compare to other one-shot imitation learning methods
    + Can we learn without expert actions
    + How well does our method scale to real world tasks

+ Methods for comparison:
    + Our method
    + Random policy: output random actions from standard normal distribution
    + Contextual policy: 
        + Input the final image of demonstration
        + Indicate goal and current image (observation)
        + Then output current action
    + LSTM:
        + Input demonstration and current observation
        + Output current action
    + LSTM + attention: only applicable to non-vision tasks

+ Task 1 : simulated reaching
    + Goal: reach a target of a particular color
    + Both vision and non-vision versions are tested
    + meta-learning can handle raw-inputs
    + Our method can handle small dataset (demonstration) well
    + Bias transformation (bt) can perform more consistently across dataset sizes

![2017-11-27 00-29-05屏幕截图.png-367.4kB][7]

+ Task 2 : simulated pushing

![2017-11-27 00-29-25屏幕截图.png-516.5kB][8]


+ Task 3 : real-world placing
    + Experiment goal : place a held item into a target container, such as a cup, plate, or bowl, while ignoring two distractors

![2017-11-27 00-29-45屏幕截图.png-795.9kB][9]

## 5. Summary and Ongoing Work (On CoRL)
+ Summary:
    + reuse prior experience when learning in new settings
    + learning-to-learn enables effective one-shot learning
+ Ongoing work: one-shot imitation from human video $\rightarrow$ during demo, let human put the ball in cup 


  [1]: http://static.zybuluo.com/VenturerXu/xk2uv5qbh24jqhpuhhfou2bo/2017-11-27%2000-11-57%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [2]: http://static.zybuluo.com/VenturerXu/wlwvfdch8136aof9gl4cn3y1/2017-12-08%2021-56-34%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [3]: http://static.zybuluo.com/VenturerXu/m9xya0rm2ycpfyqnedttct9u/2017-12-08%2022-27-01%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [4]: http://static.zybuluo.com/VenturerXu/wneggi8ilau5hmj4qb3rhjhx/2017-11-27%2000-25-32%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [5]: http://static.zybuluo.com/VenturerXu/dor4y3okiksugzirzannd405/MIL.png
  [6]: http://static.zybuluo.com/VenturerXu/un1gpjtiu95kcvyhwnaz6l54/2017-11-27%2000-28-50%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [7]: http://static.zybuluo.com/VenturerXu/2jy7768xn9md5ppkepvttzmn/2017-11-27%2000-29-05%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [8]: http://static.zybuluo.com/VenturerXu/6nh1t40henug7hatntbfflk0/2017-11-27%2000-29-25%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [9]: http://static.zybuluo.com/VenturerXu/dg1cd6tnkxfab1ptz8ie10rr/2017-11-27%2000-29-45%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png