# 1710.03641 - Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments

+ **Yunqiu Xu**

+ Other reference: 
    + [ICLR 2018 openreview][1]
    + [OpenAI blog : Meta-Learning for Wrestling][2] 

---

## 1. Introduction

+ Challenge: real world is non-stationary
    + Dynamics and objectives change over life-long time
    + Multiple learning actors

+ Traditional method: context detection and tracking
    + learn policy based on those already happened, then continuously fine-tuning for new environment
    + Limitation:
        + Inefficient
        + Limited interaction before changing $\rightarrow$ maybe insifficient to learn policy $\rightarrow$ **we need few-shot**

+ Insight: non-stationary task can be seen as a sequence of stationary tasks $\rightarrow$ change to multi-task problem

+ This paper:
    + Consider the problem of continuous adaptation to a learninging opponent in a competitive multi-agent setting
    + Design a new environment: RoboSumo
    + Dvelop gradient-based meta-learning algorithm for adaptation in dynamically changing and adversarial scenarios

## 2. Related Work
+ Life-long learning: solving multiple tasks sequentially by using already learned knowledge
+ Never-ending learning: keep a set of tasks, the set keeps growing, and the performance on all tasks keeps growing as well
+ **Continuous adaptation**: 
    + Solve a single but nonstationary task or environment
    + Learn an agent to adapt to changes of environment
    + With limited data or experience (few-shot)

## 3. Method
+ Goal: handle continuous adaptation with few-shot
+ Method: rederive MAML for multi-task, then extend to dynamically changing tasks

### 3.1 Probabilistic view of MAML
+ Here Eq.1-3 are similar to original MAML
+ A task is defined as 
$$T = \{L_T, P_T(x), P_T(x_{t+1}|x_t,a_t), H \} \space\space\space\space(1)$$

+ Inner loop: here $\phi$ is $\theta'$ in original version
    + **注意这里把公式2叫做meta-gradient update with step $\alpha$, 该公式是内循环的, 和meta-update不同**

![2017-12-12 18-16-18屏幕截图.png-9.4kB][3]

+ Meta objective: 

![2017-12-12 18-16-55屏幕截图.png-15.4kB][4]


+ Probabilistic view:
    + Task $T$ , trajectories $\tau$ and policies $\pi_{\theta}$ are random variables, $phi$ is generated from conditional distribution $P_T(\phi|\theta, \tau_{1:k})$
    + Inner loop update: equivalent to assuming the delta distribution
    $$P_T(\phi|\theta, \tau_{1:k}) := \delta(\theta - \alpha \nabla_{\theta} \frac{1}{K}\sum_{k=1}^K L(\tau_k))$$
    + Optimize meta-objective: PG where the gradient of $R_T(\theta)$

![2017-12-12 18-36-09屏幕截图.png-16.5kB][5]

### 3.2 Continuous adaptation via meta-learning
+ $D(T)$ is defined by the environment changes, and the tasks become sequentially dependent
+ Our goal: 
    + Find the dependence between consecutive tasks
    + Meta-learn a rule to minimize total expected loss during interacting
    + 这里举了个栗子, 若对手改变策略, 我们的agent也要做相应调整
    
![2017-12-12 18-20-45屏幕截图.png-119.6kB][6]

+ Here is probabilistic graph of MAML   
    + (a) MAML in a multi-task RL setting
    + (b) Extend to continuous adaptation
        + Policy and trajectories at a previous step are used to construct a new policy for the current step, i.e. $\{\phi_i, \tau_i\} \rightarrow \phi_{i+1}$
    + (c) Computation graph for the meta-update from $\phi_i$ to $\phi_{i+1}$
        + The model is optimized via truncated backpropagation through time starting from $L_{T_{i+1}}$

+ From (b) we represent nonstationary as a sequence of stationary tasks, then the goal is try to minimize the expected loss of $L$ tasks, where $L$ is a given number
![2017-12-12 18-53-03屏幕截图.png-9.9kB][7]

+ Expected loss on a pair of consecutive tasks
![2017-12-12 18-56-15屏幕截图.png-36.1kB][8]

+ To construct the parameters of policy for task $T_{i+1}$, we start from $\theta$, note that here we can also change inner step size $\alpha$
![2017-12-12 18-59-38屏幕截图.png-27.4kB][9]

+ PG for (c), as expectation is taken to both $T_i$ and $T_{i+1}$, we change (4) to
![2017-12-12 19-10-25屏幕截图.png-22.9kB][10]

### 3.3 Meta-training for Continuous Adaptation 

![2017-12-12 19-11-56屏幕截图.png-92.4kB][11]

+ **Assumption: trajectories of $T_i$ contain some infomation about $T_{i+1}$**
+ Difference from original MAML:
    + Goal: 不仅仅优化 $\theta$ 还有一系列步长 $\alpha$
    + Use a pair of consecutive tasks for training
    + During inner loop:
        + 首先基于原策略 $\pi_{\theta}$ 从 $T_i$ 中获取一系列 trajectories $\tau_{1:K}$
        + **此处存疑, 原文中说获取trajectories的过程中还需要与$T_{i+1}$互动**
        + 根据这些 $T_i$ 的 trajectories 计算 $\phi$
        + $\pi_{\phi}$ 用于解决 $T_{i+1}$, 基于该策略从 $T_{i+1}$ 中获取一个 trajectory $\tau$
    + Meta-update:
        + 通过一系列子任务获取$\tau_{1:K}$ 以及 $\tau$后, 计算$\nabla_{\theta}R_T(\theta,\alpha)$ , $\nabla_{\alpha}R_T(\theta,\alpha)$
        + 更新 $\theta$ 与 $\alpha$

### 3.4 Adaptation at Execution Time (Testing)

![2017-12-12 19-32-08屏幕截图.png-76.5kB][12]

+ Nonstationary $\rightarrow$ can not access to same task multiple times
+ How to handle : keep acting according to $\pi_{\phi}$ and re-use past experience to for computing updates of $\phi$ for each new incoming task
+ $\pi_{\phi}$ 获取的past experience与$\pi_{\theta}$会有不同 $\rightarrow$ 使用 importance weight correction 进行调整
![2017-12-12 19-40-14屏幕截图.png-18.4kB][13]

## 4. Experiment
+ A new environment : [RoboSumo][14]
+ Let robot wrestle
![2017-12-12 19-42-22屏幕截图.png-133.6kB][15]
![2017-12-12 19-46-09屏幕截图.png-71.4kB][16]

+ Result 1
![2017-12-12 19-51-47屏幕截图.png-133.5kB][17]

+ Omit some results


## 5. Conclusion
+ Regard nonstationarity as a sequence of stationary tasks
+ Train agents to exploit the dependencies between consecutive tasks 
+ During testing, the model can handle similar nonstationarities + Problem : 该工作把动态环境看作一系列静态环境的序列, 但真实环境会更复杂
+ Future work
|Current|Future|
|-----|
|One-step-ahead update of the policy|Extend to fully recurrent meta-updates $\rightarrow$ take into full history of interaction|
|(c) requires second order derivatives at training time|Utilize information provided by the loss but avoid explicit backpropagation through the gradients|
|Can't handle sparse reward $\rightarrow$ the meta-updates use policy gradients and heavily rely on the reward signal|Try to introduce auxiliary dense rewards designed to enable meta-learning|
        


  [1]: https://openreview.net/forum?id=Sk2u1g-0-
  [2]: https://blog.openai.com/meta-learning-for-wrestling/
  [3]: http://static.zybuluo.com/VenturerXu/jojsd5hr0m38nj0qenkt4oqu/2017-12-12%2018-16-18%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [4]: http://static.zybuluo.com/VenturerXu/sprknbu21mfo74l7jxac9skm/2017-12-12%2018-16-55%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [5]: http://static.zybuluo.com/VenturerXu/4blkg4bmak50db213izg3le6/2017-12-12%2018-36-09%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [6]: http://static.zybuluo.com/VenturerXu/wgaz1u5jlnzpjwayntgliupt/2017-12-12%2018-20-45%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [7]: http://static.zybuluo.com/VenturerXu/igcymdfy96s9a7p2f456f712/2017-12-12%2018-53-03%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [8]: http://static.zybuluo.com/VenturerXu/ns7k8mpq7f3w5fs8hhj0zpue/2017-12-12%2018-56-15%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [9]: http://static.zybuluo.com/VenturerXu/fkhqlh4givj3katk2mmmadvi/2017-12-12%2018-59-38%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [10]: http://static.zybuluo.com/VenturerXu/q2bq0z6ezbyi2r2fyyt4brzd/2017-12-12%2019-10-25%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [11]: http://static.zybuluo.com/VenturerXu/z7ho5xieuzuh9h5fkx37necm/2017-12-12%2019-11-56%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [12]: http://static.zybuluo.com/VenturerXu/f6jjfmvm1owdz7kg2n04cv5a/2017-12-12%2019-32-08%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [13]: http://static.zybuluo.com/VenturerXu/5gzpxp387jnu06ao9i8s1w3i/2017-12-12%2019-40-14%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [14]: https://github.com/openai/robosumo
  [15]: http://static.zybuluo.com/VenturerXu/tbpxd2e57uy015gejjgwfxfw/2017-12-12%2019-42-22%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [16]: http://static.zybuluo.com/VenturerXu/gsb0mhs7vd0t23dut74nzxyp/2017-12-12%2019-46-09%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [17]: http://static.zybuluo.com/VenturerXu/vlc60ez1mnf1eycyu673fgu2/2017-12-12%2019-51-47%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png