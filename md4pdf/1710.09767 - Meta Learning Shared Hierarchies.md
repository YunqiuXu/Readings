# 1710.09767 - Meta Learning Shared Hierarchies

+ **Yunqiu Xu**
+ Other reference:
    + [OpenAI Blog: Learning a Hierarchy][1]
    + [Meta Learning Shared Hierarchies | Two Minute Papers][2]
    + [Videos for "Meta Learning Shared Hierarchies"][3]
    + [Paper Notes : Meta Learning Shared Hierarchies][4]

-----

## 1. Introduction
+ Challenge: different tasks have different optimal policies $\rightarrow$ hard to generalize (or "transfer learning")
+ Our work: MLSH
    + Hiarchical model similar to "options framework"
    + Contain a set of shared sub-policies(primitives)
    + A task-specific master policy is used to switch these sub-tasks
    + Allow for quick learning on new tasks

+ What is a good hierarchy: find a set of low-level motor primitives that enable the high-level master policy to be learned quickly.
+ How to solve this optimization problem: 
    + Repeatedly reset the master policy to adapt the sub-policies for fast learning
    + More details can be seen in "Algorithm" part, that master policy keeps getting reset for each new task

## 2. Related Work
+ Hierarchical RL:
    + Sutton et al. [Between mdps and semi-mdps: A framework
for temporal abstraction in reinforcement learning][5] : option framework, assumes that the options are given, some recent work seeks to learn them automatically
    + Florensa et al. [Stochastic Neural Networks for Hierarchical Reinforcement Learning][6] : Use stochastic NN to learn the span of skills, the sub-policies are defined according to information-maximizing statistics
    + Bacon et al. [The option-critic architecture][7] : end-to-end learning of hierarchy through the options framework
    + Feudal Network [Dayan & Hinton 1993][8] , [DeepMind 2017][9] : learn a decomposition of complicated tasks into sub-goals (Manager & Workers)
    + Their limitation: focus on single-task, while our work uses **multi-task setting** to learn temporally extended primitives
+ Meta learning: learning to learn
    + [Dual et al. 2016][10] , [Wang et al. 2016][11] : RNN as the entire learning process
    + Mishra et al. [Meta-Learning with Temporal Convolutions][12] : Utilize temporal convolutions rather than recurrency
    + MAML [Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks][13] : Treat the test error on sampled tasks as the training error of meta-learning process, fine-tune a shared policy by optimizing through a second gradient step
    + Our difference: they try to **learn as much as possible** in a few gradient updates, we try to **learn as fast as possible** over a large number of gradient updates

## 3. Problem Statement
+ The goal of meta-learning: optimize the expected return over the sampled tasks $\rightarrow$ try to find shared parameter $psi$ which can be generalize to a new MDP

$$maximize_{\phi} E_{M \sim P_M, t = 0,...,T-1} [R]$$

+ Shared vector $\phi$:
    + Consist of a set of subvectors : $\phi_1, \phi_2, ..., \phi_K$
    + Each subvector $\phi_k$ defines a sub-policy $\pi_{\phi_k}(a|s)$
    + Parameter $\theta$ : shape stocastic policy (master policy), whose action is to choose sub-policy's index ($1,2,..,k$) every N timesteps

![2017-12-04 18-25-11屏幕截图.png-54.9kB][14]

+ By discovering a strong set of subpolicies $\phi$, we can handle new task learning by updating master policy $\theta$ only
+ Why it's faster: treat a learning problem with a horizon which is only $1/n$ times as long

## 4. Algorithm

![2017-12-04 18-36-32屏幕截图.png-87kB][15]

+ Policy update:
    + Goal: learn sub-policy parameters $\phi$
    + Sample a task $M$ and initialize master policy $\theta$
    + Warm-up period: optimize $\theta$
        + Consider the selection of a sub-policy as a single action
        + The next N timesteps, along with corresponding state changes and rewards, are viewed as a single environment transition
    + Joint update period: both $\theta$ and $\phi$ are updated
        + Treat the master policy as an extension of the environment
        + For each N -timestep slice of experience, we only update the parameters of the sub-policy that had been activated by master policy
    + For a new task: reset $\theta$ and repeat

![2017-12-04 18-37-23屏幕截图.png-120.8kB][16]
    
+ Why our method is faster:
    + Traditional meta-learning : optimize reward (master policy $\theta$) over an entire inner loop
    + MLSH : optimize $\phi$ towards maximizing reward within a single episode
    + An assumption : given fixed sub-policies $\phi$, we can learn optimal $\theta$ during warmup period
    + Only when $\theta$ is nearly optimal will $\phi$ be updated

## 5. Experiment

+ More results can be seen in https://sites.google.com/site/mlshsupplementals/

![2017-12-04 18-54-12屏幕截图.png-212kB][17]
![2017-12-04 18-55-38屏幕截图.png-142.5kB][18]

## 6. Conclusion
+ This work combines meta-learning with hierarchical RL, try to learn faster over a large number of gradient updates
+ For each task, the master policy $\theta$ is reset and learned  to be optimal with fixed sub-policies $\phi$, then nearly-optimal master policy will be treated as an extension of environment and $\phi$ will be updated
    


  [1]: https://blog.openai.com/learning-a-hierarchy/
  [2]: https://www.youtube.com/watch?v=M_eaS7X-mIw&feature=push-u&attr_tag=Prl82igzsV3rn5T6-6
  [3]: https://sites.google.com/site/mlshsupplementals/
  [4]: https://github.com/DanielTakeshi/Paper_Notes/blob/master/reinforcement_learning/Meta_Learning_Shared_Hierarchies.md
  [5]: https://www.sciencedirect.com/science/article/pii/S0004370299000521
  [6]: https://arxiv.org/abs/1704.03012
  [7]: https://arxiv.org/abs/1609.05140
  [8]: http://www.cs.toronto.edu/~fritz/absps/dh93.pdf
  [9]: https://arxiv.org/abs/1703.01161
  [10]: https://arxiv.org/abs/1611.02779
  [11]: https://arxiv.org/abs/1611.05763
  [12]: https://arxiv.org/abs/1707.03141
  [13]: https://arxiv.org/abs/1703.03400
  [14]: http://static.zybuluo.com/VenturerXu/toh90uusb6njchfledgewhr9/2017-12-04%2018-25-11%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [15]: http://static.zybuluo.com/VenturerXu/5kpr52if1jqim6a0we7tr36y/2017-12-04%2018-36-32%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [16]: http://static.zybuluo.com/VenturerXu/vk6yhninqazuie9j44w3be72/2017-12-04%2018-37-23%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [17]: http://static.zybuluo.com/VenturerXu/8ejv46ipn8qtwc3sbcaiisdk/2017-12-04%2018-54-12%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [18]: http://static.zybuluo.com/VenturerXu/q693lxsn2nxdqy5hzxpj51k7/2017-12-04%2018-55-38%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png