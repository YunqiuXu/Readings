# 1711.03817 - Learning with Options that Terminate Off-Policy

+ **Yunqiu Xu**
+ NIPS 2017 HRL Workshop

---

## 1. Introduction

+ A revision of option framework $\omega \in \Omega$:
    + Initiation set $I \subseteq S$
    + Intra-option policy $\pi_{\omega} : S \rightarrow A$ , this is **sub-policy**
    + Termination condition $\beta_{\omega} : S \rightarrow [0,1]$
    + Given a state, master policy $\pi$ select an option (suitable initiation set), then its intra-option policy will be executed to reach terminate state of this subtask $\rightarrow$ a new state for next iteration until final end

+ **Learning with longer options is more efficient, why?**
    + Termination condition $\beta$ is similar to learning rate ($\lambda$) in TD-learning 
    + Thus can make it faster to converge
+ Challenges:
    + $\beta$ will not only influent learning rate but also affect the solution
    + So if the option set is not ideal, the performance will be affected
    + **In this condition, shorter options can be better (more flowxible)**

+ Our work: 
    + Try to terminate options "off-policy"
    + **Decouple the behavior termination condition from target termination condition**
        + Behavior TC: options execute with this TC, which influence the **convergence speed**
        + Target TC: factored into the solution
    + Q($\beta$):
        + learn to evaluate a task w.r.t. options terminating off-policy
        + learn an optimal solution from suboptimal options quicker than the alternatives

## 2. Framework and Notation
### 2.1 Multi-step off-policy TD learning

+ Multi-step TD learning:
$$T_{\lambda}^{\pi}q = (1-\lambda)\sum_{t=0}^{\infty}\lambda^n(T^{\pi})^nq = q + (I - \lambda \gamma P^{\pi})^{-1}(T^{\pi}q-q)$$

+ What is off-policy learning:
    + Behavior and target policies are decoupled
    + $\pi^b \ne \pi$
    + **此处存疑, off-policy是不是类似我之前学DQN时的target net和eval net, target net用于选择动作但每隔一定步数才会更新参数**

+ Multi-step off-policy TD learning:
    + Munos et al. 2016. Safe and efficient off-policy reinforcement learning
    + Asis et al. 2017. Multi-step reinforcement learning: A unifying algorithm.

![2018-01-02 00-02-53屏幕截图.png-32.4kB][1]

### 2.2 Options
+ Similar to introduction (initiation set + option policy + terminition set), but some symbles may be different

+ **这里暂略**

## 3. Call-and-return operator

## 4. Off-policy option termination

![2018-01-02 00-15-25屏幕截图.png-97kB][2]

## 5. Experiment and Analysis

## 6. Summary

+ 本工作致力于改进option framework
    + Longer option is faster to converge but will affect performance when option set is not ideal
    + We decouple the behavior and target terminations (similar to off-policy learning)
    + Learn the solution with respect to any termination condition, regardless of how the options terminate

+ 看得比较粗略, **to be continued**
            

        


  [1]: http://static.zybuluo.com/VenturerXu/7ziklymo2uv7z8gylde95tyd/2018-01-02%2000-02-53%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [2]: http://static.zybuluo.com/VenturerXu/pqvnc0qtmz5xrmt3uyfpzi5n/2018-01-02%2000-15-25%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png