# 1802.01557 - One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning

![2018-02-11 23-32-32屏幕截图.png-59.7kB][1]

+ **Yunqiu Xu**
+ MAML $\rightarrow$ MIL $\rightarrow$ this work
    + Other ref: [机器人模仿人类动作一学就会，还能举一反三了 | 论文][2]
    + 之前搞MIL时有提及future work: 接受人类操作视频作为demonstration, 从而实现one-shot imitation via meta learning
    + 类似sim-to-real, 接受人类操作视频, 也需要考虑substantial domain shift (perspective, environment, embodiment)
    + 前人工作: 手动指定相关性, explicit human pose detection
    + 本工作: 通过meta learning学习prior knowledge, 然后在新任务实现one-shot
    + 实验: 机器人手臂 (place, push, pick-and-place)

---

## 1. Introduction
+ Challenge:
    + 机器人模仿学习与人/动物的学习过程还存在很大不同
        + 机器人需要的demonstration形式为 kinesthetic teaching 或者 teleoperation (遥控)
        + 而人类仅仅需要观看他人即可进行模仿, 且仅仅需要很少的demonstrations
    + 直接从raw visual observations进行学习存在的挑战
        + Systematic domain shift
        + A substantial amount of data
    + Prior work: 手动指定机器和人类动作行为的相关性 $\rightarrow$ 复杂, 且动作机理有区别

![2018-02-12 01-05-11屏幕截图.png-302.1kB][3]


+ Our work: let robot learn how to learn from humans from similar tasks
    + 首先利用meta-training 学习 rich prior knowledge, 其中demonstration包括人类的和遥控的
    + 机器人学习如何利用数据模仿人类动作
    + meta-training结束后, 基于之前学到的prior knowledge, 机器人可以实现one-shot imitation learning, with only one human video demonstration

+ Contribution: 应该是MIL的更进一步吧, 之前MIL是MAML和模仿学习的一个结合, 现在扩宽了模仿学习中demonstration的范围, 便于构建数据集
    

## 2. Related Work

+ Imitation learning
    
|Prior work|Our work|
|-----|
|Configuration-space trajectories level : kinesthetic teaching / teleoperation / sensors on the demonstrator|Imitate by watching human demonstrator|
|Manually resolve correspondence problem|Learn the correspondence implicitly|
|Explicit hand tracking / precise visual recognition system|Extract human's activity that are the most relevant for choosing actions|
|Explicitly determine the goal and reward, then optimize via inverseRL|Their work can not handle one-shot learning|


+ Meta learning:
    + Learning from similar tasks
    + 本工作可以认为是MAML / MIL的延伸 $\rightarrow$ MAML with domain shift between training and testing demonstration (e.g. learning from human videos)

+ Domain shift:
    + Method 1 : domain adaptation
        + Find a representation that is domain invariant
        + Vary visual domains and sim-to-real
    + Method 2 : map datapoints from one domain to another
    + Human imitation problem: 
        + Developing invariances: 光影 / 背景的变化
        + **人类和机器人行为间的physical correspondence不是invariant的, 也没法直接进行域间迁移 $\rightarrow$ 因此我们需要从视频中隐式识别人类行为的目标, 并选取相应动作**

## 3. Learning from Humans
### 3.1 Problem Setup

+ What prior knowledge should we learn:
    + Visual and physical understanding of the world
    + What kinds of outcomes the human want to achieve
    + Which actions can robot choose to get the outcome

+ Demonstrations
    + Human $d^h = <o_1, ...,o_T>$ , a sequence of image observations
    + Robot $d^r = <o_1,s_1,a_1,...,o_T,s_T,a_T>$ , image observations , robot states and robot actions
    + No assumptions about the similarty between human and robot demonstrations $\rightarrow$ can be different appearance of arms, background clutter and cameta viewpoint

+ Two phases of our approach
    + Meta-learning phase: 
        + Try to learn prior knowledge over tasks using both human and robot demonstration
        + For each training task $T_i$ , we have demonstration datasets $(D_{T_i}^h, D_{T_i}^r)$
    + Testing phase:
        + Combine prior knowledge with one human demonstration
        + Try to infer policy parameters $\phi_T$ to solve the new task

### 3.2 Domain-Adaptive Meta-Learning
+ Compared with MIL, we can not use standard imitation learning loss, since human actions are inaccessible or can not correspond to robot's actions directly
+ Adaptation objective $L_{\psi}$:
    + Does not need actions
    + Operates only on the policy actions

+ Meta training
    + Learn both policy parameters $\theta$ and adaptation parameters $\psi$ : **used for choosing actions to match robot demonstrations in $D_T^{val}$**
    + Imitation learning (behavioral cloning) objective
$$L_{BC}(\phi,d^r) = L_{BC}(\phi, \{o_{1:T}, s_{1:T}, a_{1:T}\}) = \sum_t log \pi_{\phi} (a_t|o_t,s_t)$$
    + Combine $L_{BC}$ with inner GD adaptation $\rightarrow$ meta-training objective
$$min_{\theta,\psi} \sum_{T \sim p(T)} \sum_{d^h \sim D_T^h} \sum_{d^r \sim D_T^r} L_{BC}(\theta - \alpha \nabla_{\theta} L_{\psi}(\theta, d^h), d^r )$$

![2018-02-12 01-38-35屏幕截图.png-81.5kB][4]

+ Meta testing on a new task $T$
    + Given a human demonstration $d^h$
    + Use gradient descent starting from $\theta$ with learned loss $L_{\psi}$ to infer new policy parameters
$$\phi_T = \theta - \alpha \nabla_{\theta} L_{\psi}(\theta, d^h)$$
    
![2018-02-12 01-38-43屏幕截图.png-43.3kB][5]

### 3.3 Learned Temporal Adaptation Objectives
+ Why we need this adaptation objective :     
    + 该目标可从人类视频中捕捉有用信息, 如intention或者和任务相关的对象
    + 且可以在不获取真实动作的前提下提供合适的梯度信息 $\rightarrow$ 这对于传统的BC loss (frame之间彼此独立)太难了
    + 确定demonstrate哪种行为以及哪些对象是需要的, 需要同时检测多个frame以确定人类动作
+ 我们需要引入**temporal convolutions** 来表示adaption objective $L_{\psi}$ (相关文献: 1609.03499 Wavenet: A generative model for raw audio)
    + Use multiple layers of 1D convlutions over time
    + Effective at processing temporal and sequential data
    + 我们的改进: 用类似LSTM的方式使用temporal convolutions

![2018-02-25 20-33-41屏幕截图.png-127.8kB][6]

+ MIL引入了双头结构用于one-shot imitation
    + 一个头 pre-update demonstration, 另一个头 post-update policy
    + 双头结构可被解读为在最后一层网络的某种linear loss function, 该函数生效于特定timestep
    + 计算loss和梯度: averaging over all timesteps in the demonstration
    + 在本工作中, single timestep是不够的, **因此我们工作比之前的双头结构更好?**

### 3.4 Probabilistic Interpretation
+ Adaptation的意义: GD on learned loss $L_{\psi}(\phi, D^{tr}_T)$, rather than likelihood $log p(D^{tr}_T|\phi)$

![2018-02-25 20-59-11屏幕截图.png-9.9kB][7]

+ Visual illustration of the graphical model

![2018-02-25 21-00-10屏幕截图.png-121kB][8]

## 4. Network Architectures
### 4.1 Policy $\pi$

![2018-02-25 21-02-33屏幕截图.png-180.8kB][9]

### 4.2 Learned Adaptation Objective $L_{\psi}$
+ We need to update both perception and control 
+ 将预测特征 $f$ 与policy最后一层隐藏层 $h$ 相连构建adaptation objective
+ 因此learned loss可以越过控制层被直接应用于卷积层的权重
+ 我们用之前构建的temporal adaptation objective更新关于这个任务的策略权重
$$\phi = \theta - \alpha \nabla_{\theta} L_{\psi}(\theta, d^h)$$

+ Adaptation objective 会被解构成两部分
$$L_{\psi} = L_{\psi_1}(f_{1:T}) + L_{\psi_2}(h_{1:T})$$
    + $L_{\psi_1}$ 与 $L_{\psi_2}$ 网络结构相同 (Fig 2)

## 5. Experiments
+ Questions:
    + 1. 我们的方法能否有效达到one-shot imitation with human video?
    + 2. 给定新的human demonstrator, 我们的方法能否以不同于机器人的视角泛化于human demonstration
    + 3. 和其他meta-learning方法的性能比较
    + 4. Temporal adaptation objective对于我们工作的重要性
+ Baselines:
    + Contextual policy: 输入机器观察值及人类视频的最后一帧(task), 输出预测的动作
    + DA-LSTM policy: RNN, 直接输入人类视频和机器观察值, 输出预测的动作, domain-adaptive version of Duan's work (One-shot Imitation Learning)
    + DAML, linear loss: our work with linear per-timestep adaptation obective
    + DAML, temporal loss

![2018-02-25 21-24-43屏幕截图.png-481.2kB][10]

### 5.1 PR2 Placing, Pushing, and Pick & Place

![2018-02-25 21-21-31屏幕截图.png-475.5kB][11]
![2018-02-25 21-22-02屏幕截图.png-45.4kB][12]

### 5.2 Demonstrations with Large Domain Shift
+ 不同房间环境, 不同视角

![2018-02-25 21-23-12屏幕截图.png-319.8kB][13]
![2018-02-25 21-25-03屏幕截图.png-440.2kB][14]
![2018-02-25 21-23-34屏幕截图.png-55.2kB][15]

### 5.3 Sawyer Experiments
+ 在不同机器人和不同机器demonstration集合类型中的泛化性

![2018-02-25 21-26-19屏幕截图.png-328.6kB][16]

### 5.4 Learned Adaptation Objective Ablation

![2018-02-25 21-26-43屏幕截图.png-38kB][17]

## 6. Summary
+ 在MIL基础上进行了延伸, 通过构造adaptation objective, 令机器人可以接受人类视频实现one-shot imitation learning
+ 不足:
    + Meta-test 时的任务和训练时的任务很相似, 在未来的工作中, 我们希望能够处理 unseen objects and demonstrators
    + The amount of demonstration per object is too low than general imitation learning, 在未来的工作中我们会尝试构建更泛化的机器人 (对一个任务可以有更多更宽泛的demonstration) **加噪音可否实现这一功能?**

            


  [1]: http://static.zybuluo.com/VenturerXu/hwohjccpgl37ecr8fvh0fze9/2018-02-11%2023-32-32%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [2]: https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247494193&idx=2&sn=28df37a58275979700a4a6042a0b8b31&chksm=e8d05d43dfa7d45596e774475ab08cc7a721501e403a246795b804f63d0e63cf478deedf13e6&mpshare=1&scene=1&srcid=020702QlUhMRi9KewG7gL4ku&key=965c657e27ac1763410e181275f3d1039b34fbd41de97818347ee2dccd9c62f6840e132cda3faea8e7003505c7810941976cc19e642aa8c2c251c480b07ed62fe138914419b4514456029c34a6b8a7a3&ascene=0&uin=MTY1NzgxNjIyMg==&devicetype=iMac%20MacBookAir7,2%20OSX%20OSX%2010.11.6%20build%2815G19009%29&version=12020810&nettype=WIFI&lang=zh_CN&fontScale=100&pass_ticket=jyHQZASkWqF9%2bpCagtpb2vwLYBuJqhklllLM%2bshFAUo9I1ewqVtqCY68RDPXPK0v
  [3]: http://static.zybuluo.com/VenturerXu/qw38lrd2pbyqn5n1f1d42r0l/2018-02-12%2001-05-11%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [4]: http://static.zybuluo.com/VenturerXu/raicxb5f8nvjxu6g2u5tuq98/2018-02-12%2001-38-35%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [5]: http://static.zybuluo.com/VenturerXu/vf05utc61n3ek4tchdrz1r7v/2018-02-12%2001-38-43%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [6]: http://static.zybuluo.com/VenturerXu/ytfbgjc2bf2zgl7k2l5v78zv/2018-02-25%2020-33-41%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [7]: http://static.zybuluo.com/VenturerXu/57rgr791wrxqfo1rh465p2kf/2018-02-25%2020-59-11%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [8]: http://static.zybuluo.com/VenturerXu/o1wa9dpajepkagcw1dzl2wjh/2018-02-25%2021-00-10%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [9]: http://static.zybuluo.com/VenturerXu/j78r8hpvhholwx9lfeyqx2cb/2018-02-25%2021-02-33%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [10]: http://static.zybuluo.com/VenturerXu/05i5x2mbrhvy0qwon4nkgvhr/2018-02-25%2021-24-43%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [11]: http://static.zybuluo.com/VenturerXu/5e9alj3zksgcu03jca5ozb3v/2018-02-25%2021-21-31%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [12]: http://static.zybuluo.com/VenturerXu/h1wofalurz22cddlm0hee99r/2018-02-25%2021-22-02%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [13]: http://static.zybuluo.com/VenturerXu/6b4m5xbar0f32fxa807jzjnc/2018-02-25%2021-23-12%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [14]: http://static.zybuluo.com/VenturerXu/b7qz8ozpj0v4fkhr1t7flsdx/2018-02-25%2021-25-03%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [15]: http://static.zybuluo.com/VenturerXu/zqw5dwvtofe3bm2t9d98s01p/2018-02-25%2021-23-34%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [16]: http://static.zybuluo.com/VenturerXu/y68u46hnp2m0m0xz4rmrbjlc/2018-02-25%2021-26-19%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [17]: http://static.zybuluo.com/VenturerXu/asj7nzh6d3vs6v9fv6quyofu/2018-02-25%2021-26-43%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png