# 1802.04821 - Evolved Policy Gradients

![2018-02-27 11-01-17屏幕截图.png-26kB][1]

+ **Yunqiu Xu**
+ Brief introduction
    + Pieter Abbeel 组关于 meta-learning 的新作品, 用于基于梯度的强化学习
    + 主要思想: 
        + Envolve a differentiable loss function, which is parameterized via temporal convolutions over agent's experience
        + 基于meta-learning学一个loss function
    + 这里引入的时序卷积在DAML中也被提及, 在DAML中:             
        + 时序卷积被用于构造adaptation objective $L_{\psi}$, 应该也是一种 loss function 吧
        + The parameters are updated via $\phi = \theta - \alpha \nabla_{\theta} L_{\psi}(\theta, d^h)$
        + Further reading: [1802.01557 - One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning][2]      
    + Preformance:
        + Learns faster on several randomized environments
        + During testing, optimized only learned loss function, **does not require explicit reward signal**

---
## 1. Introduction

+ Challenge: 
    + Sometimes we can not obtain explicit internal rewards but simple feedback from teacher ([1706.03741 - Deep Reinforcement Learning from Human Preferences][3])
    + In the real world, there's no teacher

+ Insight:
    + Get prior knowledge over tasks to make learning faster on a new task
    + How to encode knowledge: **through envolved loss function**

+ Our method:
    + Meta learning:
        + 内循环: 选取一个任务并优化外循环提供的loss function
        + 外循环: 在经过内循环之后, 优化loss function的参数, 使其能够最大化给定object的回报
    + Why envolved policy gradients
        + 虽然内循环可用SGD优化, 但优化外循环较困难 $\rightarrow$ outer objective无法被显式表示为loss parameters的函数
        + 因此我们引入进化策略 (blackbox optimizer), 将 evolved loss L 看作参数化的 PG $\frac{\partial_L}{\partial_{\pi}}$
    + Advantages to introduce learned loss function
        + Encode prior knowledge
        + Optimize the true objective rather than short-term returns (which will lead to local optima)

+ Our contribution:
    + Formulating a meta-learning approach that learns a differentiable loss function for RL agents
    + Optimizing the parameters of this loss function via ES, overcoming the challenge that final returns are not explicit functions of the loss parameters
    + Designing a loss architecture that takes into account agent history via temporal convolutions
    + Demonstrating that the learned loss function can train agents that achieve higher returns than agents trained via an off-the-shelf policy gradient method

## 2. Related Work
+ RL
    + MDP: 这里引用的Sutton的教材
    + PG:
    $$L_{pg} = E[R_{\tau} log \pi(\tau)] = E\left[ R_{\tau} \sum_{t=0}^H log \pi (a_t|s_t) \right]$$
    + AC: change $L_{pg}$ as 
    $$L_{ac} = E\left[ \sum_{t=0}^H A(s_t) log \pi (a_t|s_t) \right]$$
    + Our method: **does not use hand-defind function such as $L_{ac}$, learns a loss function instead**

+ MAML: 和我们工作相比, 原始的MAML框架限制过多, 且要求目标函数可微
+ DAML: 
    + MAML及MIL的延伸, 和本工作类似也引入了temporal convolutions
    + 不同:
        + 他们工作重在解决behavioral cloning存在的问题(无法使用人类视频指导机器), 需要demonstration, 而我们的工作不需要
        + DAML的目标函数同样受限于可微, 其内循环仅有一步SGD, 而我们的目标函数是long-horizon且不可微的, 因此我们的内循环可以运行更多timesteps
    + **此处存疑: 本工作的summary中提到通过meta-learning学习到了一个可微的loss function, 且基于大量环境动作序列, 这不就是用了demonstration么**

+ 另外两篇工作和我们工作类似, 也在构建reward function上做文章
    + [1706.03741 - Deep Reinforcement Learning from Human Preferences][4]: 使用人的feedback来加速学习过程, 并尝试解决一些难以定义reward function的问题
    + [Inverse RL][5]: 尝试从demonstration中反向推导出reward

## 3. Methodology
+ 本工作学到的loss function包含基于以往任务的时序卷积, 该loss function一方面可以内化(internalizing)环境回报, 另一方面可以激励较好的行为(如exploration)
+ $\phi$: parameters of learned loss function
+ $\pi_{\theta}$: train agent's policy 

### 3.1 Meta-learning Objective
+ Inner loop:
![2018-02-27 20-27-30屏幕截图.png-11.5kB][6]
+ Outer loop:
![2018-02-27 20-27-40屏幕截图.png-11.6kB][7]

### 3.2 Algorithm
+ 一方面我们难以用$L_{\phi}$的显式函数表示最终返回的$R_{\tau}$, 另一方面 Eq.3. 无法用基于GD的方法直接来解, 因此我们用基于进化策略的如下算法对外循环的loss function进行优化

![2018-02-27 20-31-28屏幕截图.png-142.9kB][8]

+ 内循环: 在每个trajectory结束后, 使用GD来优化loss function $L_i$, 并将返回值 $R_i$ 传递给外循环

![2018-02-27 20-31-45屏幕截图.png-6.2kB][9]

+ 外循环: 根据所有的内循环返回值 $\{R_i\}_{i=1}^n$ 来更新 $\phi$

![2018-02-27 20-31-54屏幕截图.png-7.6kB][10]

### 3.3 Architecture

![2018-02-27 20-41-49屏幕截图.png-146.5kB][11]

![2018-02-27 20-49-24屏幕截图.png-16.1kB][12]

## 4. Experiment
+ 基于MuJoCo的随机连续控制环境Hopper以及Reacher
![2018-02-27 20-42-40屏幕截图.png-62.6kB][13]
+ 本工作与PPO进行性能比较

### 4.1 Is the learned loss effective

![2018-02-27 20-47-44屏幕截图.png-172.4kB][14]
![2018-02-27 20-48-33屏幕截图.png-100.9kB][15]

### 4.2 Does the loss encourage smooth learning

![2018-02-27 20-49-58屏幕截图.png-58.8kB][16]

### 4.3 Does the loss encourage exploration
+ Forward-backward Hopper environment: 每个环境或者给予向前跳跃reward, 或者给向后跳跃
+ Learning curve: learned loss is able to train agents that exhibit exploratory behavior

![2018-02-27 20-52-49屏幕截图.png-165kB][17]

+ The qualitative behavior of agent during learning

![2018-02-27 20-53-09屏幕截图.png-118.1kB][18]

### 4.4 How sensitive is the loss to different kinds of inputs

![2018-02-27 20-53-41屏幕截图.png-183.9kB][19]    

## 5. Summary
+ 本文工作:
    + 基于meta-learning, 从大量环境动作序列中学到可微的loss function
    + 和手动设计相比, 该loss function是adaptive(快速学习新任务)以及instructive的(在测试时不需要环境提供reward, 例如其中一个实验我们不能仅仅根据观察值得到reward)
    + Why instructive: 该性质可被理解为loss function对训练过程中见过的reward结构的内化, 我的理解是, **对于一个新任务, 如果以前见过类似任务, 便可以大致猜出这个新任务的reward**

+ 未来工作:
    + 本工作需要序列化学习(在更新i+1前需要先通过外循环更新i), 在未来工作中可以进行并行化来提升效率
    + 效率提升后, 可尝试更有挑战性的任务
        


  [1]: http://static.zybuluo.com/VenturerXu/t49iny50r8xy0zbnzb4tfdgs/2018-02-27%2011-01-17%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [2]: https://arxiv.org/abs/1802.01557
  [3]: https://arxiv.org/abs/1706.03741
  [4]: https://arxiv.org/abs/1706.03741
  [5]: https://dl.acm.org/citation.cfm?id=657801
  [6]: http://static.zybuluo.com/VenturerXu/dumkhfxuunjjinnw20icr6zk/2018-02-27%2020-27-30%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [7]: http://static.zybuluo.com/VenturerXu/11vzfv0fwulqo3ft80mtcey0/2018-02-27%2020-27-40%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [8]: http://static.zybuluo.com/VenturerXu/k5be8flhnvbrtgmi6y3udg1t/2018-02-27%2020-31-28%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [9]: http://static.zybuluo.com/VenturerXu/r80pxl7l40vn1uonzv19knrx/2018-02-27%2020-31-45%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [10]: http://static.zybuluo.com/VenturerXu/rovie5x7pmhb0ctfspgx68r5/2018-02-27%2020-31-54%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [11]: http://static.zybuluo.com/VenturerXu/4acse1ynmf2su4kqa7o0jvqu/2018-02-27%2020-41-49%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [12]: http://static.zybuluo.com/VenturerXu/zmqgdfrhsqmehy5yc07qbwuy/2018-02-27%2020-49-24%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [13]: http://static.zybuluo.com/VenturerXu/t1eii16cgy1cn82lr2ufru7r/2018-02-27%2020-42-40%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [14]: http://static.zybuluo.com/VenturerXu/6zvc7b7i30hskaaugjyc68zd/2018-02-27%2020-47-44%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [15]: http://static.zybuluo.com/VenturerXu/uo406umf8r4qh20s49pszlya/2018-02-27%2020-48-33%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [16]: http://static.zybuluo.com/VenturerXu/fd4arzzqc6z3oli1vdg2ds1z/2018-02-27%2020-49-58%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [17]: http://static.zybuluo.com/VenturerXu/m0ts6h7ocx2rmf6njo2ozq56/2018-02-27%2020-52-49%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [18]: http://static.zybuluo.com/VenturerXu/rqjmv09hcs2qdvc7uvayursw/2018-02-27%2020-53-09%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png
  [19]: http://static.zybuluo.com/VenturerXu/tulwhh1nhyi9kgx4m0uanpr6/2018-02-27%2020-53-41%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png